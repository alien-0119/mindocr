{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mindocr","title":"MindOCR","text":""},{"location":"#introduction","title":"Introduction","text":"<p>MindOCR is an open-source toolbox for OCR development and application based on MindSpore, which integrates series of mainstream text detection and recognition algorihtms/models, provides easy-to-use training and inference tools. It can accelerate the process of developing and deploying SoTA text detection and recognition models in real-world applications, such as DBNet/DBNet++ and CRNN/SVTR, and help fulfill the need of image-text understanding.</p>  Major Features  <ul> <li>Modular design: We decoupled the OCR task into several configurable modules. Users can setup the training and evaluation pipelines, customize the data processing pipeline and model architectures easily by modifying just few lines of code.</li> <li>High-performance: MindOCR provides a series of pretrained weights trained with optimized configurations that reach competitive performance on OCR tasks.</li> <li>Low-cost-to-apply: Easy-to-use inference tools are provided in MindOCR to perform text detection and recognition tasks.</li> </ul> <p>The following is the corresponding <code>mindocr</code> versions and supported mindspore versions.</p> mindocr mindspore master master 0.4 2.3.0 0.3 2.2.10 0.1 1.8"},{"location":"#installation","title":"Installation","text":"Details"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>MindOCR is built on MindSpore AI framework and is compatible with the following framework versions. installation guideline for Training, please refer to the installation links shown below.</p> <ul> <li>mindspore [install] Please install correct MindSpore version refer to <code>mindocr</code> versions.</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (for distributed training/evaluation)  [install]</li> </ul> <p>MindSpore Lite offline Inference please refer to Lite offline Environment Installation</p>"},{"location":"#dependency","title":"Dependency","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#install-from-source-recommend","title":"Install from Source (recommend)","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>Using <code>-e</code> for \"editable\" mode can help resolve potential module import issues.</p>"},{"location":"#install-from-docker","title":"Install from docker","text":"Details   The environment information of dockers provided is as following:  - OS\uff1aEuler2.8  - CANN\uff1a7.0  - Python\uff1a3.9  - MindSpore\uff1a2.2.10  - MindSpore Lite\uff1a2.2.10  Please follow the steps to install docker\uff1a  1. Download docker     - 910\uff1a         <pre><code>docker pull swr.cn-central-221.ovaijisuan.com/mindocr/mindocr_dev_910_ms_2_2_10_cann7_0_py39:v1\n</code></pre>     - 910*:         <pre><code>docker pull swr.cn-central-221.ovaijisuan.com/mindocr/mindocr_dev_ms_2_2_10_cann7_0_py39:v1\n</code></pre> 2. Create container     <pre><code>docker_name=\"temp_mindocr\"\n# 910\nimage_name=\"swr.cn-central-221.ovaijisuan.com/mindocr/mindocr_dev_910_ms_2_2_10_cann7_0_py39:v1\"\n# 910*\nimage_name=\"swr.cn-central-221.ovaijisuan.com/mindocr/mindocr_dev_ms_2_2_10_cann7_0_py39:v1\"\n\ndocker run --privileged --name ${docker_name} \\\n    --tmpfs /tmp \\\n    --tmpfs /run \\\n    -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\\n    --device=/dev/davinci1 \\\n    --device=/dev/davinci2 \\\n    --device=/dev/davinci3 \\\n    --device=/dev/davinci4 \\\n    --device=/dev/davinci5 \\\n    --device=/dev/davinci6 \\\n    --device=/dev/davinci7 \\\n    --device=/dev/davinci_manager \\\n    --device=/dev/hisi_hdc \\\n    --device=/dev/devmm_svm \\\n    -v /etc/localtime:/etc/localtime \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    --shm-size 800g \\\n    --cpus 96 \\\n    --security-opt seccomp=unconfined \\\n    --network=bridge -itd ${image_name} bash\n</code></pre>  3. Enter container     <pre><code># set docker id\ncontainer_id=\"your docker id\"\ndocker exec -it --user root $container_id bash\n</code></pre>  4. Set environment variables     After entering container, set environment variables by the following command\uff1a     <pre><code>source env_setup.sh\n</code></pre>"},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install mindocr\n</code></pre> <p>As this project is under active development, the version installed from PyPI is out-of-date currently. (will update soon).</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-text-detection-and-recognition-demo","title":"1. Text Detection and Recognition Demo","text":"<p>After installing MindOCR, we can run text detection and recognition on an arbitrary image easily as follows.</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n                                          --det_algorithm DB++  \\\n                                          --rec_algorithm CRNN  \\\n                                          --visualize_output True\n</code></pre> <p>After running, the results will be saved in <code>./inference_results</code> by default. Here is an example result.</p> <p> </p> <p>  Visualization of text detection and recognition result  </p> <p>We can see that all texts on the image are detected and recognized accurately. For more usage, please refer to the inference section in tutorials.</p>"},{"location":"#2-model-training-and-evaluation-quick-guideline","title":"2. Model Training and Evaluation - Quick Guideline","text":"<p>It is easy to train your OCR model with the <code>tools/train.py</code> script, which supports both text detection and recognition model training.</p> <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <p>The <code>--config</code> arg specifies the path to a yaml file that defines the model to be trained and the training strategy including data process pipeline, optimizer, lr scheduler, etc.</p> <p>MindOCR provides SoTA OCR models with their training strategies in <code>configs</code> folder. You may adapt it to your task/dataset, for example, by running</p> <pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/dbpp_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre> <p>Similarly, it is easy to evaluate the trained model with the <code>tools/eval.py</code> script.</p> <pre><code>python tools/eval.py \\\n    --config {path/to/model_config.yaml} \\\n    --opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre> <p>For more illustration and usage, please refer to the model training section in Tutorials.</p>"},{"location":"#3-model-offline-inference","title":"3. Model Offline Inference","text":"<p>You can do MindSpore Lite inference in MindOCR using MindOCR models or Third-party models (PaddleOCR, MMOCR, etc.). Please refer to Model Offline Inference Tutorial</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Datasets<ul> <li>Dataset Preparation</li> <li>Data Transformation Mechanism</li> </ul> </li> <li>Model Training<ul> <li>Yaml Configuration</li> <li>Text Detection</li> <li>Text Recognition</li> <li>Distributed Training</li> <li>Advance: Gradient Accumulation, EMA, Resume Training, etc</li> </ul> </li> <li>Inference with MindSpore<ul> <li>Python Online Inference</li> </ul> </li> <li>Inference with MindSpore Lite<ul> <li>Model Offline Inference Tutorial</li> </ul> </li> <li>Developer Guides<ul> <li>Customize Dataset</li> <li>Customize Data Transformation</li> <li>Customize a New Model</li> <li>Customize Postprocessing Method</li> </ul> </li> </ul>"},{"location":"#model-list","title":"Model List","text":"Text Detection <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021)</li> </ul> Text Recognition <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> MASTER (PR'2019)</li> <li> VISIONLAN (ICCV'2021)</li> <li> RobustScanner (ECCV'2020)</li> <li> ABINet (CVPR'2021)</li> </ul> Layout Analysis <ul> <li> YOLOv8 (Ultralytics Inc.)</li> </ul> Key Information Extraction <ul> <li> LayoutXLM (arXiv'2021)</li> <li> LayoutLMv3 (arXiv'2022)</li> </ul> Table Recognition <ul> <li> TableMaster (arXiv'2021)</li> </ul> OCR large model <ul> <li> Vary (arXiv'2023)</li> </ul> <p>For the detailed performance of the trained models, please refer to https://github.com/mindspore-lab/mindocr/blob/main/configs.</p> <p>For details of MindSpore Lite and ACL inference models support, please refer to MindOCR Models Support List and Third-party Models Support List (PaddleOCR etc.).</p>"},{"location":"#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li>Born-Digital Images [download]</li> <li>CASIA-10K [download]</li> <li>CCPD [download]</li> <li>Chinese Text Recognition Benchmark paper(datasets/chinese_text_recognition.md)]</li> <li>COCO-Text [download]</li> <li>CTW [download]</li> <li>ICDAR2015 paper(datasets/icdar2015.md)]</li> <li>ICDAR2019 ArT [download]</li> <li>LSVT [download]</li> <li>MLT2017 paper(datasets/mlt2017.md)]</li> <li>MSRA-TD500 paper(datasets/td500.md)]</li> <li>MTWI-2018 [download]</li> <li>RCTW-17 [download]</li> <li>ReCTS [download]</li> <li>SCUT-CTW1500 paper(datasets/ctw1500.md)]</li> <li>SROIE [download]</li> <li>SVT [download]</li> <li>SynText150k paper(datasets/syntext150k.md)]</li> <li>SynthText paper(datasets/synthtext.md)]</li> <li>TextOCR [download]</li> <li>Total-Text paper(datasets/totaltext.md)]</li> </ul> Layout Analysis Datasets <ul> <li>PublayNet paper(https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz)]</li> </ul> Key Information Extraction Datasets <ul> <li>XFUND paper(https://github.com/doc-analysis/XFUND/releases/tag/v1.0)]</li> </ul> Table Recognition Datasets <ul> <li>PubTabNet paper(https://dax-cdn.cdn.appdomain.cloud/dax-pubtabnet/2.0.0/pubtabnet.tar.gz)]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Frequently asked questions about configuring environment and mindocr, please refer to FAQ.</p>"},{"location":"#notes","title":"Notes","text":""},{"location":"#what-is-new","title":"What is New","text":"News <ul> <li> <p>2023/04/01 1. Add new trained models</p> <ul> <li>LayoutLMv3 for key information extraction</li> </ul> </li> <li> <p>2024/03/20 1. Add new trained models</p> <ul> <li>Vary-toy for OCR large model, providing Qwen-1.8B LLM-based object detection and OCR abilities</li> </ul> </li> <li> <p>2023/12/25 1. Add new trained models</p> <ul> <li>TableMaster for table recognition 2. Add more benchmark datasets and their results</li> <li>PubTabNet</li> </ul> </li> <li> <p>2023/12/14 1. Add new trained models</p> <ul> <li>LayoutXLM for key information extraction</li> <li>VI-LayoutXLM for key information extraction</li> <li>PP-OCRv3 DBNet for text detection and PP-OCRv3 SVTR for recognition, supporting online inferece and finetuning 2. Add more benchmark datasets and their results</li> <li>XFUND 3. Multiple specifications support for Ascend 910: DBNet ResNet-50, DBNet++ ResNet-50, CRNN VGG7, SVTR-Tiny, FCENet, ABINet</li> </ul> </li> <li>2023/11/28 1. Add offline inference support for PP-OCRv4<ul> <li>PP-OCRv4 DBNet for text detection and PP-OCRv4 CRNN for text recognition, supporting offline inferece 2. Fix bugs of third-party models offline inference</li> </ul> </li> <li>2023/11/17 1. Add new trained models<ul> <li>YOLOv8 for layout analysis 2. Add more benchmark datasets and their results</li> <li>PublayNet</li> </ul> </li> <li>2023/07/06 1. Add new trained models<ul> <li>RobustScanner for text recognition</li> </ul> </li> <li>2023/07/05 1. Add new trained models<ul> <li>VISIONLAN for text recognition</li> </ul> </li> <li>2023/06/29 1. Add new trained models<ul> <li>FCENet for text detection</li> <li>MASTER for text recognition</li> </ul> </li> <li> <p>2023/06/07 1. Add new trained models</p> <ul> <li>PSENet for text detection</li> <li>EAST for text detection</li> <li>SVTR for text recognition 2. Add more benchmark datasets and their results</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. Add resume training function, which can be used in case of unexpected interruption in training. Usage: add the <code>resume</code> parameter under the <code>model</code> field in the yaml config, e.g.,<code>resume: True</code>, load and resume training from {ckpt_save_dir}/train_resume.ckpt or <code>resume: /path/to/train_resume.ckpt</code>, load and resume training from the given path. 4. Improve postprocessing for detection: re-scale detected text polygons to original image space by default, which can be enabled by add \"shape_list\" to the <code>eval.dataset.output_columns</code> list. 5. Refactor online inference to support more models, see README.md for details.</li> </ul> </li> <li> <p>2023/05/15 1. Add new trained models</p> <ul> <li>DBNet++ for text detection</li> <li>CRNN-Seq2Seq for text recognition</li> <li>DBNet pretrained on SynthText is now available: checkpoint url 2. Add more benchmark datasets and their results</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>More benchmark results for DBNet are reported here. 3. Add checkpoint manager for saving top-k checkpoints and improve log. 4. Python inference code refactored. 5. Bug fix: use Meter to average loss for large datasets, disable <code>pred_cast_fp32</code> for ctcloss in AMP training, fix error when invalid polygons exist.</li> </ul> </li> <li> <p>2023/05/04 1. Support loading self-defined pretrained checkpoints via setting <code>model-pretrained</code> with checkpoint url or local path in yaml. 2. Support setting probability for executing augmentation including rotation and flip. 3. Add Exponential Moving Average(EMA) for model training, which can be enabled by setting <code>train-ema</code> (default: False) and <code>train-ema_decay</code> in the yaml config. 4. Arg parameter changed\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: change the column number feeding into the network to the column index. 5. Arg parameter changed\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: change the column number corresponds to the label to the column index.</p> </li> <li> <p>2023/04/21 1. Add parameter grouping to support flexible regularization in training. Usage: add <code>grouping_strategy</code> argument in yaml config to select a predefined grouping strategy, or use <code>no_weight_decay_params</code> argument to pick layers to exclude from weight decay (e.g., bias, norm). Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 2. Add gradient accumulation to support large batch size training. Usage: add <code>gradient_accumulation_steps</code> in yaml config, the global batch size = batch_size * devices * gradient_accumulation_steps. Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 3. Add gradient clip to support training stablization. Enable it by setting <code>grad_clip</code> as True in yaml config.</p> </li> <li> <p>2023/03/23 1. Add dynamic loss scaler support, compatible with drop overflow update. To enable dynamic loss scaler, please set <code>type</code> of <code>loss_scale</code> as <code>dynamic</code>. A YAML example can be viewed in <code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. Arg names changed: <code>output_keys</code> -&gt; <code>output_columns</code>, <code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code> 2. Data pipeline updated</p> </li> <li> <p>2023/03/13 1. Add system test and CI workflow. 2. Add modelarts adapter to allow training on OpenI platform. To train on OpenI:     <pre><code>    i)   Create a new training task on the openi cloud platform.\n    ii)  Link the dataset (e.g., ic15_mindocr) on the webpage.\n    iii) Add run parameter `config` and write the yaml file path on the website UI interface, e.g., '/home/work/user-job-dir/V0001/configs/rec/test.yaml'\n    iv)  Add run parameter `enable_modelarts` and set True on the website UI interface.\n    v)   Fill in other blanks and launch.\n</code></pre></p> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all kinds of contributions including issues and PRs to make MindOCR better.</p> <p>Please refer to CONTRIBUTING.md for the contributing guideline. Please follow the Model Template and Guideline for contributing a model that fits the overall interface :)</p>"},{"location":"#license","title":"License","text":"<p>This project follows the Apache License 2.0 open-source license.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider citing:</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"datasets/borndigital/","title":"Born-Digital Images Dataset","text":""},{"location":"datasets/borndigital/#data-downloading","title":"Data Downloading","text":"<p>The Born-Digital dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>This dataset is divided into 4 tasks: (1.1) Text Localization, (1.2) Text Segmentation, (1.3) Word Recognition, and  (1.4) End To End.  For now, we consider and download only the dataset for Task 1.1.</p> <p>After downloading the images and annotations, unzip the files and rename as appropriate e.g. <code>train_images</code> for the images and <code>train_labels</code> for the ground truths, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>Born-Digital\n  |--- train_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- train_labels\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/borndigital/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/borndigital/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name borndigital --task det \\\n    --image_dir path/to/Born-Digital/train_images/ \\\n    --label_dir path/to/Born-Digital/train_labels \\\n    --output_path path/to/Born-Digital/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>Born-Digital/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/casia10k/","title":"CASIA-10K Dataset","text":""},{"location":"datasets/casia10k/#data-downloading","title":"Data Downloading","text":"<p>The CASIA-10K dataset can be downloaded from this link.</p> <p>After downloading the file as above, unzip it, after which the directory structure should be like as follows (ignoring the archive file):</p> <pre><code>CASIA-10K\n  |--- test\n  |    |--- PAL00001.jpg\n  |    |--- PAL00001.txt\n  |    |--- PAL00005.jpg\n  |    |--- PAL00005.txt\n  |    |--- ...\n  |--- train\n  |    |--- PAL00003.jpg\n  |    |--- PAL00003.txt\n  |    |--- PAL00006.jpg\n  |    |--- PAL00006.txt\n  |    |--- ...\n  |--- CASIA-10K_test.txt\n  |--- CASIA-10K_train.txt\n</code></pre>"},{"location":"datasets/casia10k/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/casia10k/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name casia10k --task det \\\n    --image_dir path/to/CASIA-10K/train/ \\\n    --label_dir path/to/CASIA-10K/train \\\n    --output_path path/to/CASIA-10K/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>CASIA-10K/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/ccpd/","title":"Chinese City Parking Dataset (CCPD) 2019","text":""},{"location":"datasets/ccpd/#data-downloading","title":"Data Downloading","text":"<p>The CCPD can be downloaded from this link using either the Google or BaiduYun drive links. This dataset is divided into 3 sets: train, val, test. Labels for each set can be found under the <code>splits</code> directory of the dataset. CCPD-Green dataset is already separated into different folders and thus does not require labels.</p> <p>The annotations for each image are embedded into the filename of the image. The format is described on their official website here.</p> <p>After downloading the dataset, the directory structure should be like as follows: <pre><code>CCPD2019\n  |--- ccpd_base\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- ccpd_blur\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- ...\n  |--- ...\n  |--- ...\n  |--- splits\n</code></pre></p>"},{"location":"datasets/ccpd/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ccpd/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ccpd --task det \\\n    --image_dir path/to/CCPD2019/ccpd_base \\\n    --label_dir path/to/CCPD2019/splits/train.txt \\\n    --output_path path/to/CCPD2019/det_gt.txt\n</code></pre> <p><code>label_dir</code> is not required for CCPD-Green dataset.</p> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>CCPD2019/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/chinese_text_recognition/","title":"Chinese-Text-Recognition","text":""},{"location":"datasets/chinese_text_recognition/#data-downloading","title":"Data Downloading","text":"<p>Following the setup in Benchmarking-Chinese-Text-Recognition, we use the same training, validation and evaliation data as described in Datasets section.</p> <p>Please download the following LMDB files as introduced in Downloads section:</p> <ul> <li>scene datasets: The union dataset contains RCTW, ReCTS, LSVT, ArT, CTW</li> <li>web: MTWI</li> <li>document: generated with Text Render</li> <li>handwriting dataset: SCUT-HCCDoc</li> </ul>"},{"location":"datasets/chinese_text_recognition/#data-structure","title":"Data Structure","text":"<p>After downloading the files, please put all training files under the same folder <code>training</code>, all validation data under <code>validation</code> folder, and all evaluation data under <code>evaluation</code>.</p> <p>The data structure should be like:</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#data-configuration","title":"Data Configuration","text":"<p>To use the datasets, you can specify the datasets as follow in configuration file.</p>"},{"location":"datasets/chinese_text_recognition/#model-training","title":"Model Training","text":"<pre><code>...\ntrain:\n  ...\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\n    data_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\n    data_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n  ...\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#model-evaluation","title":"Model Evaluation","text":"<pre><code>...\ntrain:\n  # NO NEED TO CHANGE ANYTHING IN TRAIN SINCE IT IS NOT USED\n...\neval:\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\n    data_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n  ...\n</code></pre> <p>Back to dataset converters</p>"},{"location":"datasets/cocotext/","title":"COCO-Text Dataset","text":""},{"location":"datasets/cocotext/#data-downloading","title":"Data Downloading","text":"<p>Official Website</p> <p>The COCO-Text images dataset and the annotations (in JSON format) <code>annotations v1.4 JSON</code> can be downloaded from this link.</p> <p>Note: Please register an account to download this dataset.</p> <p>After downloading the images and annotations, unzip the files, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>COCO-Text\n  |--- train_images\n  |    |--- COCO_train2014_000000000036.jpg\n  |    |--- COCO_train2014_000000000064.jpg\n  |    |--- ...\n  |--- COCO_Text.json\n</code></pre></p>"},{"location":"datasets/cocotext/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/cocotext/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name cocotext --task det \\\n    --image_dir path/to/COCO-Text/train_images/ \\\n    --label_dir path/to/COCO-Text/COCO_Text.json \\\n    --output_path path/to/COCO-Text/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>COCO-Text/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/converters/","title":"Dataset Preparation","text":"<p>This document shows how to convert ocr annotation to the general format (not including LMDB) for model training.</p> <p>You may also refer to <code>convert_datasets.sh</code> which is a quick solution for converting annotation files of all datasets under a given directory.</p> To download and convert OCR datasets to the required data format, please refer to these instructions. <ul> <li>Born-Digital Images</li> <li>CASIA-10K</li> <li>CCPD</li> <li>Chinese text recognition</li> <li>COCO-Text</li> <li>CTW</li> <li>ICDAR2015</li> <li>ICDAR2019 ArT</li> <li>LSVT</li> <li>MLT2017</li> <li>MSRA-TD500</li> <li>MTWI-2018</li> <li>RCTW-17</li> <li>ReCTS</li> <li>SCUT-CTW1500</li> <li>SROIE</li> <li>SVT</li> <li>SynText150k</li> <li>SynthText</li> <li>TextOCR</li> <li>Total-Text</li> </ul>"},{"location":"datasets/converters/#text-detectionspotting-annotation","title":"Text Detection/Spotting Annotation","text":"<p>The format of the converted annotation file should follow: <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>Taking ICDAR2015 (ic15) dataset as an example, to convert the ic15 dataset to the required format, please run</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task det \\\n        --image_dir /path/to/ic15/det/train/ch4_training_images \\\n        --label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n        --output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task det \\\n        --image_dir /path/to/ic15/det/test/ch4_test_images \\\n        --label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n        --output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"datasets/converters/#text-recognition-annotation","title":"Text Recognition Annotation","text":""},{"location":"datasets/converters/#common-dataset-format","title":"Common Dataset Format","text":"<p>The annotation format for text recognition dataset follows <pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> Note that image name and text label are seperated by \\t.</p> <p>To convert, please run: <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task rec \\\n        --label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt \\\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task rec \\\n        --label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt \\\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"datasets/converters/#lmdb-dataset-format","title":"LMDB Dataset Format","text":"<p>Some of the dataset can be converted to LMDB format. Currently, this is only supported for the <code>SynthText</code> and <code>SynthAdd</code> datasets.</p> <p>To convert to LMDB format, please run</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name synthtext \\\n    --task rec_lmdb \\\n    --image_dir /path/to/SynthText \\\n    --label_dir /path/to/SynthText_gt.mat \\\n    --output_path ST_full\n</code></pre>"},{"location":"datasets/ctw/","title":"CTW Dataset","text":""},{"location":"datasets/ctw/#data-downloading","title":"Data Downloading","text":"<p>The CTW images dataset Official Website | Download Link</p> <p>Note: Please fill in a form to download this dataset.</p> <p>The images are in 26 batches, i.e. 26 different .tar archived files of the format <code>images-trainval/ctw-trainval*.tar</code>. All 26 batches need to be downloaded.</p> <p>The CTW annotations (in JSON Lines format i.e. <code>.jsonl</code>) can be downloaded from this download link. The annotations archived file is named <code>ctw-annotations.tar.gz</code>.</p> <p>After downloading the zipped images, unzip the batches and collect all the images into a single folder e.g. <code>train_val/</code>. After downloading the zipped annotations, unzip them. Finally, the directory structure should look like this (ignoring the archive files):</p> <pre><code>CTW\n  |--- train_val\n  |    |--- 0000172.jpg\n  |    |--- 0000174.jpg\n  |    |--- ...\n  |--- train.jsonl\n  |--- val.jsonl\n  |--- test_cls.jsonl\n  |--- info.json\n</code></pre>"},{"location":"datasets/ctw/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ctw/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw --task det \\\n    --image_dir path/to/CTW/train_val/ \\\n    --label_dir path/to/CTW/train.jsonl \\\n    --output_path path/to/CTW/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>CTW/</code>.</p> <p>Note that the <code>label_dir</code> flag may be altered to prepare the validation data.</p> <p>Back to dataset converters</p>"},{"location":"datasets/ctw1500/","title":"SCUT-CTW1500 Datasets","text":""},{"location":"datasets/ctw1500/#data-downloading","title":"Data Downloading","text":"<p>SCUT-CTW1500 Datasets download link</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/ctw1500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ctw1500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw1500 --task det \\\n    --image_dir path/to/ctw1500/train_images/ \\\n    --label_dir path/to/ctw1500/ctw_1500_train_labels \\\n    --output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw1500 --task det \\\n    --image_dir path/to/ctw1500/test_images/ \\\n    --label_dir path/to/ctw1500/gt_ctw_1500 \\\n    --output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>ctw1500/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/ic19_art/","title":"ICDAR2019 Dataset","text":""},{"location":"datasets/ic19_art/#data-downloading","title":"Data Downloading","text":"<p>The ICDAR2019 ArT images and annotations Official Website | Download Link</p> <p>Note: Please register an account to download this dataset</p> <p>For the images, the archived file <code>train_images.tar.gz</code> from the section \"Task 1 and Task 3\" needs to be downloaded. For the annotations, the .JSON file <code>train_labels.json</code> from the same section needs to be downloaded.</p> <p>After downloading the dataset, unzip the files, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>ICDAR2019-ArT\n  |--- train_images\n  |    |--- train_images\n  |    |    |--- gt_0.jpg\n  |    |    |--- gt_1.jpg\n  |    |    |--- ...\n  |--- train_labels.json\n</code></pre></p>"},{"location":"datasets/ic19_art/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ic19_art/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ic19_art --task det \\\n    --image_dir path/to/ICDAR2019-ArT/train_images/train_images/ \\\n    --label_dir path/to/ICDAR2019-ArT/train_labels.json \\\n    --output_path path/to/ICDAR2019-ArT/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>ICDAR2019-ArT/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/icdar2015/","title":"ICDAR 2015 Datasets","text":""},{"location":"datasets/icdar2015/#data-downloading","title":"Data Downloading","text":"<p>ICDAR 2015 Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>ICDAR 2015 Challenge has three tasks. Task 1 is Text Localization. Task 3 is Word Recognition. Task 4 is End-to-end Text Spotting. Task 2 Text Segmentation is not available.</p>"},{"location":"datasets/icdar2015/#text-localization","title":"Text Localization","text":"<p>The four files downloaded for task 1 are <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre></p>"},{"location":"datasets/icdar2015/#word-recognition","title":"Word Recognition","text":"<p>The three files downloaded for task 3 are <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre> The three files are only needed for training word recognition models. Training text detection models does not require the three files.</p>"},{"location":"datasets/icdar2015/#e2e","title":"E2E","text":"<p>The nine files downloaded for task 4 are the union of the four files in the text localization task (task 1) and five vocabulary files <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre> If you download a file named <code>Challenge4_Test_Task4_GT.zip</code>, please note that it is the same file as <code>Challenge4_Test_Task1_GT.zip</code>, except for its name. In this repository, we will use <code>Challenge4_Test_Task4_GT.zip</code> for ICDAR2015 dataset.</p> <p>After downloading the icdar2015 dataset, place all the files under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"datasets/lsvt/","title":"LSVT Dataset","text":""},{"location":"datasets/lsvt/#data-downloading","title":"Data Downloading","text":"<p>The LSVT dataset Official Website |  Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>The images are split into two zipped files <code>train_full_images_0.tar.gz</code> and <code>train_full_images_1.tar.gz</code>. Both are to be downloaded. After downloading the images as above, unzip them and collect them into a single folder e.g. <code>train_images</code>.</p> <p>The LSVT annotations (in JSON format) can be downloaded from this download link. The file <code>train_full_labels.json</code> needs to be downloaded.</p> <p>After downloading the images and annotations as above, the directory structure should be like as follows (ignoring the archive files): <pre><code>LSVT\n  |--- train_images\n  |    |--- gt_0.jpg\n  |    |--- gt_1.jpg\n  |    |--- ...\n  |--- train_full_labels.json\n</code></pre></p>"},{"location":"datasets/lsvt/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/lsvt/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name lsvt --task det \\\n    --image_dir path/to/LSVT/train_images/ \\\n    --label_dir path/to/LSVT/train_full_labels.json \\\n    --output_path path/to/LSVT/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>LSVT/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/mlt2017/","title":"MLT 2017 Datasets","text":""},{"location":"datasets/mlt2017/#data-downloading","title":"Data Downloading","text":"<p>MLT (Multi-Lingual) 2017 dataset Paper | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>MLT 2017 dataset consists of two tasks. Task 1 is Text detection (Multi-Language Script) and Task 2 is Word Recognition.</p>"},{"location":"datasets/mlt2017/#text-detectionmulti-script","title":"Text Detection(Multi-script)","text":"<p>The 11 files downloaded for task 1 are</p> <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre> <p>No need to download the Test Set.</p>"},{"location":"datasets/mlt2017/#word-identification","title":"Word Identification","text":"<p>The 6 files downloaded for task 2 are <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n\n&lt;/details&gt;\n\nAfter downloading the files, place them under `[path-to-data-dir]` folder:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip</p> <p>```</p> <p>Back to dataset converters</p>"},{"location":"datasets/mtwi2018/","title":"ICPR MTWI-2018 Dataset","text":""},{"location":"datasets/mtwi2018/#data-downloading","title":"Data Downloading","text":"<p>The ICPR MTWI-2018 dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>The ICPR MTWI dataset has derived three tasks: Text Line(column) Recognition of Web Images, Text Detection of Web Images, and End to End Text Detection and Recognition of Web Images. The three tasks share the same training data: <code>mtwi_train.zip</code>; For test data, task1 has test data: <code>mtwi_task1.zip</code>, and task\u2154 share the same test data: <code>mtwi_task2_3.zip</code>. For now, we will consider and download only the training data <code>mtw_train.zip</code>.</p> <p>After downloading the dataset, unzip the file, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>MTWI-2018\n  |--- image_train\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- txt_train\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/mtwi2018/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/mtwi2018/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name mtwi2018 --task det \\\n    --image_dir path/to/MTWI-2018/image_train/ \\\n    --label_dir path/to/MTWI-2018/txt_train.json \\\n    --output_path path/to/MTWI-2018/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>MTWI-2018/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/pubtabnet/","title":"PubTabNet Dataset","text":""},{"location":"datasets/pubtabnet/#data-downloading","title":"Data Downloading","text":"<p>PubTabNet dataset Official Website | Download Link</p> <p>Download the images and annotations, unzip the files. The directory structure should be:</p> <pre><code>pubtabnet\n  |--- train\n  |    |--- PMC1064074_007_00.png\n  |    |--- PMC1064076_003_00.png\n  |    |--- ...\n  |--- test\n  |    |--- PMC1064127_003_00.png\n  |    |--- PMC1065052_003_00.png\n  |    |--- ...\n  |--- val\n  |    |--- PMC1064865_002_00.png\n  |    |--- PMC1079806_002_00.png\n  |    |--- ...\n  |--- PubTabNet_2.0.0.jsonl\n</code></pre>"},{"location":"datasets/pubtabnet/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/pubtabnet/#for-table-recognition-task","title":"For Table Recognition Task","text":"<p>To prepare the annotation for Table Recognition, run the following commands:</p> <ul> <li>Split the annotation for training set:</li> </ul> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name pubtabnet --task table \\\n    --image_dir path/to/pubtabnet/train/ \\\n    --label_dir path/to/pubtabnet/PubTabNet_2.0.0.jsonl \\\n    --output_path path/to/pubtabnet/pubtab_train.jsonl \\\n    --split train\n</code></pre> <ul> <li>Split the annotation for validation set:</li> </ul> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name pubtabnet --task table \\\n    --image_dir path/to/pubtabnet/val/ \\\n    --label_dir path/to/pubtabnet/PubTabNet_2.0.0.jsonl \\\n    --output_path path/to/pubtabnet/pubtab_val.jsonl \\\n    --split val\n</code></pre> <ul> <li>Note: the annotation for testing set is not provided</li> </ul> <p>Then, the generated standard annotation file <code>pubtab_train.jsonl</code> and <code>pubtab_val.jsonl</code> will be placed under the folder <code>pubtabnet/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/rctw17/","title":"RCTW-17 Dataset","text":""},{"location":"datasets/rctw17/#data-downloading","title":"Data Downloading","text":"<p>The RCTW dataset Official Website | Download Link</p> <p>The training set is split into two zip files <code>train_images.zip.001</code> and <code>train_images.zip.002</code>. The annotations are <code>*_gts.zip</code> files.</p> <p>After downloading and unzipping the images and annotations, collect the images into a single folder e.g. <code>train_images/</code>, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>RCTW-17\n  |--- train_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- train_gts\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/rctw17/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/rctw17/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name rctw17 --task det \\\n    --image_dir path/to/RCTW-17/train_images/ \\\n    --label_dir path/to/RCTW-17/train_gts \\\n    --output_path path/to/RCTW-17/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>RCTW-17/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/rects/","title":"ReCTS Dataset","text":""},{"location":"datasets/rects/#data-downloading","title":"Data Downloading","text":"<p>The ReCTS images and annotations dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>After downloading the images and annotations, unzip the file, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>ReCTS\n  |--- img\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- gt\n  |    |--- &lt;image_name&gt;.json\n  |    |--- &lt;image_name&gt;.json\n  |    |--- ...\n  |--- gt_unicode\n  |    |--- &lt;image_name&gt;.json\n  |    |--- &lt;image_name&gt;.json\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/rects/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/rects/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name rects --task det \\\n    --image_dir path/to/ReCTS/img/ \\\n    --label_dir path/to/ReCTS/gt_unicode.json \\\n    --output_path path/to/ReCTS/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>ReCTS/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/sroie/","title":"SROIE Dataset","text":""},{"location":"datasets/sroie/#data-downloading","title":"Data Downloading","text":"<p>The SROIE dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>This dataset is divided into 3 tasks: (1) Text Localisation, (2) OCR, and (3) Key Information Extraction. For now, we consider and download the updated dataset only for Task 1.</p> <p>After downloading and unzipping the dataset as above and renaming the extracted folder as appropriate e.g. <code>train</code>, the directory structure should be like as follows (ignoring the archive files): <pre><code>SROIE\n  |--- train\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/sroie/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/sroie/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name sroie --task det \\\n    --image_dir path/to/SROIE/train/ \\\n    --label_dir path/to/SROIE/train \\\n    --output_path path/to/SROIE/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>SROIE/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/svt/","title":"The Street View Text Dataset (SVT)","text":""},{"location":"datasets/svt/#data-downloading","title":"Data Downloading","text":"<p>The Street View Text Dataset (SVT) Download Link</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"datasets/svt/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/svt/#for-recognition-task","title":"For Recognition task","text":"<p>To prepare the data for text recognition, you can run the following command:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name  svt --task rec \\\n    --image_dir path/to/svt1/ \\\n    --label_dir path/to/svt1/train.xml \\\n    --output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>Then you can have a folder <code>cropped_images/</code> and an annotation file <code>rec_train_gt.txt</code> under the folder <code>svt1/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/syntext150k/","title":"SynText150k Datasets","text":""},{"location":"datasets/syntext150k/#data-downloading","title":"Data Downloading","text":"<p>SynText150k paper</p> <p>Download Syntext-150k - Part1: 54,327 [images][annotations] - Part2: 94,723 [images][annotations]</p> <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"datasets/synthtext/","title":"SynthText Datasets","text":""},{"location":"datasets/synthtext/#data-downloading","title":"Data Downloading","text":"<p>SynthText is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout.</p> <p>Paper | Download Link</p> <p>Download the <code>SynthText.zip</code> file and unzip in <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: Additionally, It is strongly recommended to pre-process the <code>SynthText</code> dataset before using it as it contains some faulty data: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat --image_dir=/path-to-data-dir/SynthText\n</code></pre> This operation will generate a filtered output in the same format as the original <code>SynthText</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/td500/","title":"MSRA Text Detection 500 Database (MSRA-TD500)","text":""},{"location":"datasets/td500/#data-downloading","title":"Data Downloading","text":"<p>MSRA Text Detection 500 Database\uff08MSRA-TD500\uff09Download Link</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/td500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/td500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name td500 --task det \\\n    --image_dir path/to/MSRA-TD500/train/ \\\n    --label_dir path/to/MSRA-TD500/train \\\n    --output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name td500 --task det \\\n    --image_dir path/to/MSRA-TD500/test/ \\\n    --label_dir path/to/MSRA-TD500/test \\\n    --output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>MSRA-TD500/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/textocr/","title":"TextOCR Dataset","text":""},{"location":"datasets/textocr/#data-downloading","title":"Data Downloading","text":"<p>The TextOCR dataset Official Website | Download Link</p> <p>After downloading the images and annotations, unzip the files, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>TextOCR\n  |--- train_val_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- TextOCR_0.1_train.json\n  |--- TextOCR_0.1_val.json\n</code></pre></p>"},{"location":"datasets/textocr/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/textocr/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name textocr --task det \\\n    --image_dir path/to/TextOCR/train_val_images/ \\\n    --label_dir path/to/TextOCR/TextOCR_0.1_train.json \\\n    --output_path path/to/TextOCR/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>TextOCR/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/totaltext/","title":"Total-Text Datasets","text":""},{"location":"datasets/totaltext/#data-downloading","title":"Data Downloading","text":"<ul> <li> <p>Total-Text paper</p> </li> <li> <p>github repo\uff1a</p> </li> <li>images</li> <li> <p>annotations</p> </li> <li> <p>Dataset download link\uff1a</p> </li> <li>images (size = 441Mb)</li> <li>annotations (.txt format)</li> </ul> <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"inference/convert_dynamic/","title":"Convert dynamic","text":""},{"location":"inference/convert_dynamic/#inference-dynamic-shape-scaling","title":"Inference - Dynamic Shape Scaling","text":""},{"location":"inference/convert_dynamic/#1-introduction","title":"1. Introduction","text":"<p>In some inference scenarios, such as object recognition after detection, the input batch size and image size of the recognition network are not fixed because the number of object and the size of the object are not fixed. If each inference is calculated according to the maximum batch size or maximum image size, it will cause a waste of computing resources.</p> <p>Therefore, you can set some candidate values during model conversion, and resize to the best matching candidate value during inference, thereby improving performance. Users can manually select these candidate values empirically, or they can be statistically derived from the dataset.</p> <p>This tool integrates the function of dataset statistics, can count the appropriate combination of <code>batch size</code>, <code>height</code> and <code>width</code> as candidate values, and encapsulates the model conversion tool, thus realizing the automatic model shape scaling.</p>"},{"location":"inference/convert_dynamic/#2-environment","title":"2. Environment","text":"<p>Please refer to Environment Installation to install MindSpore Lite environment.</p>"},{"location":"inference/convert_dynamic/#3-model","title":"3. Model","text":"<p>Currently, ONNX model files are supported, and by MindSpore Lite, they are automatically shape scaling and converted to MIndIR model files.</p> <p>Please make sure that the input model is the dynamic shape version. For example, if the text detection model needs to shape scaling for H and W, make sure that at least the H and W axes are dynamic, and the shape can be <code>(1,3,-1,-1)</code> and <code>(-1,3,- 1,-1)</code>etc.</p>"},{"location":"inference/convert_dynamic/#4-dataset","title":"4. Dataset","text":"<p>Two types of data are supported:</p> <ol> <li>Image folder</li> </ol> <ul> <li> <p>This tool will read all the images in the folder, record <code>height</code> and <code>width</code>, and count suitable candidate values</p> </li> <li> <p>Suitable for text detection and text recognition models</p> </li> </ul> <ol> <li>Annotation file for text detection</li> </ol> <ul> <li> <p>Refer to converter, which is the annotation file output when the      parameter <code>task</code> is <code>det</code></p> </li> <li> <p>This tool will read the coordinates of the text box marked under each image, record <code>height</code> and <code>width</code>, and the      number of boxes as <code>batch size</code>, and count suitable candidate values</p> </li> <li> <p>Suitable for text recognition models</p> </li> </ul>"},{"location":"inference/convert_dynamic/#5-usages","title":"5. Usages","text":"<p><code>cd deploy/models_utils/auto_scaling</code></p>"},{"location":"inference/convert_dynamic/#51-command-example","title":"5.1 Command example","text":"<ul> <li>auto shape scaling for batch size</li> </ul> <pre><code>python converter.py \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/det_gt.txt\n    --input_shape=-1,3,48,192 \\\n    --output_path=output\n</code></pre> <p>The output is a single MindIR model: <code>model_dynamic_bs.mindir</code></p> <ul> <li>auto shape scaling for height and width</li> </ul> <pre><code>python converter.py \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/images \\\n    --input_shape=1,3,-1,-1 \\\n    --output_path=output\n</code></pre> <p>The output is a single MindIR model: <code>model_dynamic_hw.mindir</code></p> <ul> <li>auto shape scaling for batch size, height and width</li> </ul> <pre><code>python converter.py \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/images \\\n    --input_shape=-1,3,-1,-1 \\\n    --output_path=output\n</code></pre> <p>The output result is multiple OM models, combining multiple different batch sizes, and each model uses the same dynamic image size\uff1a<code>model_dynamic_bs1_hw.mindir</code>, <code>model_dynamic_bs4_hw.mindir</code>, ......</p> <ul> <li>no shape scaling</li> </ul> <pre><code>python converter.py \\\n    --model_path=/path/to/model.onnx \\\n    --input_shape=4,3,48,192 \\\n    --output_path=output\n</code></pre> <p>The output is a single MindIR model: <code>model_static.mindir</code></p>"},{"location":"inference/convert_dynamic/#52-parameter-details","title":"5.2 Parameter Details","text":"Name Default Required Description model_path None Y Path to model file input_shape None Y model input shape, NCHW format data_path None N Path to image folder or annotation file input_name x N model input name backend lite N converter backend, lite or acl output_path ./output N Path to output model soc_version Ascend310P3 N soc_version for Ascend\uff0cAscend310P3 or Ascend310"},{"location":"inference/convert_dynamic/#53-configuration-file","title":"5.3 Configuration file","text":"<p>In addition to the above command line parameters, there are some parameters in auto_scaling.yaml to describe the statistics of the dataset, You can modify it yourself if necessary:</p> <ul> <li>limit_side_len</li> </ul> <p>The size limit of <code>height</code> and <code>width</code> of the original input data, if it exceeds the range, it will be compressed   according to the ratio, and the degree of discreteness of the data can be adjusted.</p> <ul> <li>strategy</li> </ul> <p>Data statistics algorithm strategy, supports <code>mean_std</code> and <code>max_min</code> two algorithms, default: <code>mean_std</code>.</p> <ul> <li>mean_std</li> </ul> <pre><code>mean_std = [mean - n_std * sigma\uff0cmean + n_std * sigma]\n</code></pre> <ul> <li>max_min</li> </ul> <pre><code>max_min = [min - (max - min) * expand_ratio / 2\uff0cmax + (max - min) * expand_ratio / 2]\n</code></pre> <ul> <li>width_range/height_range</li> </ul> <p>For the width/height size limit after discrete statistics, exceeding will be filtered.</p> <ul> <li>interval</li> </ul> <p>Interval size, such as some networks may require that the input size must be a multiple of 32.</p> <ul> <li>max_scaling_num</li> </ul> <p>The maximum number of discrete values for shape scaling .</p> <ul> <li>batch_choices</li> </ul> <p>The default batch size value, if the data_path uses an image folder, the batch size information cannot be counted, and   the default value will be used.</p> <ul> <li>default_scaling</li> </ul> <p>If user does not use data_path, provide default <code>height</code> and <code>width</code> discrete values for shape scaling .</p>"},{"location":"inference/convert_tutorial/","title":"Model Converter Tutorial","text":"<p>The tutorial includes the process of converting the trained model into the 'MindSpore Lite MindIR' used for inference. The process is as follows:</p> <pre><code>graph LR;\n    A[MindOCR models] -- export --&gt; B[MindIR] -- converter_lite --&gt; C[MindSpore Lite MindIR];\n    D[PaddleOCR train models] -- export --&gt; E[PaddleOCR infer models] -- paddle2onnx --&gt; F[ONNX]-- converter_lite --&gt; C;</code></pre> <ul> <li>MindOCR checkpoint -&gt; MindSpore MindIR -&gt; MindSpore Lite MindIR;</li> <li>PaddleOCR train model -&gt; ONNX -&gt; MindSpore Lite MindIR;</li> </ul>"},{"location":"inference/convert_tutorial/#1-model-export","title":"1. Model Export","text":"<p>This chapter includes the process of exporting MindIR or ONNX files of training models.</p> <p>Some models provide download links for MIndIR/ONNX export files, as shown in MindOCR Models List, PPOCR Models List.</p>"},{"location":"inference/convert_tutorial/#11-mindocr-model-export","title":"1.1 MindOCR Model Export","text":"<p>Export a MindIR file by checkpoint file after training. Please execute <code>tools/export.py</code>:</p> <pre><code># export static shape `crnn_resnet34` model MindIR\npython tools/export.py --model_name_or_config configs/rec/crnn/crnn_resnet34.yaml --model_type rec --local_ckpt_path ~/.mindspore/models/crnn_resnet34-83f37f07.ckpt --data_shape 32 100\n\n# export dynamic grading `crnn_resnet34` model MindIR\npython tools/export.py --model_name_or_config configs/rec/crnn/crnn_resnet34.yaml --model_type rec --local_ckpt_path ~/.mindspore/models/crnn_resnet34-83f37f07.ckpt --is_dynamic_shape True\n\nFor more details on usage, please execute `python tools/export.py -h`.\n</code></pre> <p>Some parameter descriptions:</p> <ul> <li>model_name_or_config: Name of the model to be converted, or the path to the model YAML config file.</li> <li>model_type: Model type, support [\"det\", \"rec\", \"cls\"].</li> <li>local_ckpt_path: Path to a local checkpoint. If set, export mindir by loading local ckpt. Otherwise, export mindir by downloading online ckpt.</li> <li>data_shape: The data shape [H, W] for exporting mindir files. Required when arg <code>is_dynamic_shape</code> is False. It is recommended to be the same as the rescaled data shape in evaluation to get the best inference performance.</li> <li>is_dynamic_shape: Whether the export data shape is dynamic or static.</li> </ul>"},{"location":"inference/convert_tutorial/#12-paddleocr-model-export","title":"1.2 PaddleOCR Model Export","text":"<p>PaddleOCR have two formats of Paddle models, training model and inference model, with the following differences:</p> Model Type Model format Introduction Training Model .pdparams\u3001.pdopt\u3001.states PaddlePaddle training model which save model weights and optimizer states Inference Model inference.pdmodel\u3001inference.pdiparams PaddlePaddle inference model which save model weights and network structure <p>Download the model file and extract it. Please distinguish whether it is a training model or an inference model based on the model format.</p>"},{"location":"inference/convert_tutorial/#121-training-model-inference-model","title":"1.2.1 Training Model -&gt; Inference Model","text":"<p>In the download link of PaddleOCR model, there are two formats: trained model and inference model. If a training model is provided, it needs to be converted to the format of inference model.</p> <p>On the original PaddleOCR introduction page of each trained model, there are usually conversion script samples that only need to input the config file, model file, and save path of the trained model. The example is as follows:</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n    -c configs/det/det_r50_vd_db.yml \\\n    -o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\n    Global.save_inference_dir=./det_db\n</code></pre>"},{"location":"inference/convert_tutorial/#122-inference-model-onnx","title":"1.2.2 Inference Model -&gt; ONNX","text":"<p>Install model conversion tool paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>For detailed usage tutorials, please refer to Paddle2ONNX model transformation and prediction\u3002</p> <p>Run the conversion command to generate the onnx model:</p> <pre><code>paddle2onnx \\\n    --model_dir det_db \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file det_db.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <p>The <code>input_shape_dict</code> in the parameter can generally be viewed by opening the inference model using the Netron, or found in the code in tools/export_model. py above.</p>"},{"location":"inference/convert_tutorial/#2-mindspore-lite-mindir-convert","title":"2. MindSpore Lite MindIR Convert","text":"<p>You need to use the <code>converter_lite</code> tool to convert the above exported MindIR file offline so that it can be used for MindSpore Lite inference.</p> <p>The tutorial for the <code>converter_lite</code> command can be referred to Offline Conversion of Inference Models.</p> <p>Assuming the input model is input.mindir and the output model after <code>converter_lite</code> conversion is output.mindir, the conversion command is as follows:</p> <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk={fmk} \\\n    --optimize=ascend_oriented \\\n    --modelFile=model.mindir \\\n    --outputFile=model_lite \\\n    --configFile=config.txt\n</code></pre> <p>FMK is the original format of the input model, which can be MindIR or ONNX.</p> <p><code>config.txt</code> is the configuration path for extended settings. In MindOCR, <code>config.txt</code> can be used to set dynamic shape format and precision mode. We will discuss it in detail in the next chapter.</p>"},{"location":"inference/convert_tutorial/#21-shape-format-settings","title":"2.1 Shape Format Settings","text":""},{"location":"inference/convert_tutorial/#211-static-shape","title":"2.1.1 Static Shape","text":"<p>If the input name of the exported model is <code>x</code>, and the input shape is <code>(1,3,736,1280)</code>, then the <code>config.txt</code> is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>The generated output.mindir is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p>"},{"location":"inference/convert_tutorial/#212-dynamic-shapescaling","title":"2.1.2 Dynamic Shape(scaling)","text":"<p>Note: ascend 310 not support dynamic shape.</p> <p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number and size of targets is not fixed resulting. If each inference is computed at the maximum Batch Size or maximum Image Size, it will result in wasted computational resources.</p> <p>Assuming the exported model input shape is (-1, 3, -1, -1), and the NHW axes are dynamic. Therefore, some optional values can be set during model conversion to adapt to input images of various size during inference.</p> <p><code>converter_lite</code> achieves this by setting the <code>dynamic_dims</code> parameter in <code>[ascend_context]</code> through <code>--configFile</code>. Please refer to the Dynamic Shape Configuration for details. We will refer to it as Model Shape Scaling for short.</p> <p>So, there are two options for conversion, by setting different config.txt:</p> <ul> <li> <p>Dynamic Image Size</p> <p>N uses fixed values, HW uses multiple optional values, the config.txt is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> </li> <li> <p>Dynamic Batch Size</p> <p>N uses multiple optional values, HW uses fixed values, the config.txt is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> </li> </ul> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Note:</p> <p>If the exported model is a static shape version, it cannot infer dynamic shape, and it is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial/#22-model-precision-mode-setting","title":"2.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>converter_lite</code> when converting the model. Please refer to the Ascend Conversion Tool Description, the usage of <code>precision_mode</code> parameter is described in the table of the configuration file, you can choose <code>enforce_fp16</code>, <code>enforce_fp32</code>, <code>preferred_fp32</code> and <code>enforce_origin</code> etc. So, you can add the <code>precision_mode</code> parameter in the <code>[Ascend_context]</code> of the above config.txt file to set the precision mode:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>If not set, defaults to <code>enforce_fp16</code>.</p>"},{"location":"inference/environment/","title":"Environment","text":""},{"location":"inference/environment/#offline-inference-environment-installation","title":"Offline Inference Environment Installation","text":"<p>This tutorial only covers the environment installation of MindOCR for offline inference on Atlas 300 series inference devices.</p>"},{"location":"inference/environment/#1-version-matching-table","title":"1. Version Matching Table","text":"<p>Please refer to the version matching table when setting up the inference environment. It is recommended to use MindSpore 2.2.14 for inference.</p> <p>The version of Driver and Firmware is different for different chips. Please download the matched driver and firmware according to the CANN package version.</p> <p>Now, we taking Atlas 300I Inference Card (Model: 3010) run on x86 CPU as an example, introduce version matching relationship. The following installation version is also introduced using this as an example.</p> MindSpore Driver Firmware CANN MindOCR 2.2.14 23.0.0 7.1.0.3.220 7.0.0.beta1 v0.4.0 <p>Other MindSpore and Ascend software version matching please refer to MindSpore Install.</p>"},{"location":"inference/environment/#2-ascend-environment-installation","title":"2. Ascend Environment Installation","text":"<p>There are two versions of the Ascend software package, the commercial edition and the community edition. The commercial edition is only for commercial customers and download is restricted; The community edition can be freely downloaded, and the following examples all use the community edition.</p> <p>This example uses the Ascend package that comes with MindSpore 2.2.14, other MindSpore version please refer to Installing Ascend AI processor software package.</p> software version package name download Driver 23.0.0 A300-3010-npu-driver_23.0.0_linux-x86_64.run link Firmware 7.1.0.3.220 A300-3010-npu-firmware_7.1.0.3.220.run link CANN nnae 7.0.0.beta1 Ascend-cann-nnae_7.0.0_linux-x86_64.run link CANN kernels(Optional) 7.0.0.beta1 Ascend-cann-kernels-310p_7.0.0_linux.run link"},{"location":"inference/environment/#install","title":"Install","text":"<pre><code># Note: When installing a new machine, install the driver first and then the firmware.\n# When the scenario of upgrade, install the firmware first and then the driver.\nbash A300-3010-npu-driver_23.0.0_linux-x86_64.run --full\nbash A300-3010-npu-firmware_7.1.0.3.220.run --full\nbash Ascend-cann-nnae_7.0.0_linux-x86_64.run --install\nbash Ascend-cann-kernels-310p_7.0.0_linux.run --install\n\npip uninstall te topi hccl -y\npip install sympy\npip install /usr/local/Ascend/nnae/latest/lib64/te-*-py3-none-any.whl\npip install /usr/local/Ascend/nnae/latest/lib64/hccl-*-py3-none-any.whl\nreboot\n</code></pre>"},{"location":"inference/environment/#configure-environment-variables","title":"Configure Environment Variables","text":"<pre><code>source /usr/local/Ascend/nnae/set_env.sh\n</code></pre>"},{"location":"inference/environment/#3-mindspore-install","title":"3. MindSpore Install","text":"<pre><code>pip install mindspore==2.2.14\n\n# Check version number, offline inference MindSpore only uses CPU\npython -c \"import mindspore;mindspore.set_context(device_target='CPU');mindspore.run_check()\"\n</code></pre>"},{"location":"inference/environment/#4-mindspore-lite-install","title":"4. MindSpore Lite Install","text":"software version package name download Inference Toolkit 2.2.14 mindspore-lite-2.2.14-linux-{arch}.tar.gz link Python Wheel 2.2.14 mindspore_lite-2.2.14-{python_version}-linux_{arch}.whl link <p>Unzip the inference toolkit and pay attention to setting environment variables:</p> <pre><code>tar -xvf mindspore-lite-2.2.14-linux-{arch}.tar.gz\ncd mindspore-lite-2.2.14-linux-{arch}/\nexport LITE_HOME=${PWD}    # The actual path after extracting the tar package\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> <p>If interface with python, install the required whl package using pip.</p> <pre><code>pip install mindspore_lite-2.2.14-{python_version}-linux_{arch}.whl\n</code></pre>"},{"location":"inference/environment/#5-mindocr-install","title":"5. MindOCR Install","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>Using '- e' to enter editable mode, help solve import issues.</p>"},{"location":"inference/inference_tutorial/","title":"MindOCR Offline Inference","text":""},{"location":"inference/inference_tutorial/#introduction","title":"Introduction","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite inference backend, integrates text detection, angle classification, and text recognition, implements end-to-end OCR inference process, and optimizes inference performance using pipeline parallelism.</p> <p>MindOCR supported models can find in MindOCR models list\uff0cPPOCR models list, You can jump to the models list page to download MindIR/ONNX for converting MindSpore Lite offline models.</p> <p>The overall process of MindOCR Lite inference is as follows:</p> <pre><code>graph LR;\n    A[MindOCR models] -- export --&gt; B[MindIR] -- converter_lite --&gt; C[MindSpore Lite MindIR];\n    D[ThirdParty models] -- xx2onnx --&gt; E[ONNX] -- converter_lite --&gt; C;\n    C --input --&gt; F[MindOCR Infer] -- outputs --&gt; G[Evaluation];\n    H[images] --input --&gt; F[MindOCR Infer];</code></pre>"},{"location":"inference/inference_tutorial/#2-environment-instalation","title":"2. Environment Instalation","text":"<p>Please refer to Offline Inference Environment Installation.</p>"},{"location":"inference/inference_tutorial/#3-model-conversion","title":"3. Model conversion","text":"<p>Please refer to Model Converter Tutorial.</p>"},{"location":"inference/inference_tutorial/#4-inference-python","title":"4. Inference (Python)","text":"<p>Enter the inference directory\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"inference/inference_tutorial/#41-detection-classification-recognition","title":"4.1 Detection + Classification + Recognition","text":"<pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=det_cls_rec \\\n    --vis_pipeline_save_dir=det_cls_rec\n</code></pre> <p>Note: set <code>--character_dict_path=/path/to/xxx_dict.txt</code> if not only use numbers and lowercase.</p> <p>The visualization images are stored in det_cls_rec, as shown in the picture.</p> <p> </p> <p> Visualization of text detection and recognition result </p> <p>The results are saved in det_cls_rec/pipeline_results.txt in the following format:</p> <pre><code>img_182.jpg [{\"transcription\": \"cocoa\", \"points\": [[14.0, 284.0], [222.0, 274.0], [225.0, 325.0], [17.0, 335.0]]}, {...}]\n</code></pre>"},{"location":"inference/inference_tutorial/#42-detection-recognition","title":"4.2 Detection + Recognition","text":"<p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=det_rec \\\n    --vis_pipeline_save_dir=det_rec\n</code></pre> <p>Note: set <code>--character_dict_path=/path/to/xxx_dict.txt</code> if not only use numbers and lowercase.</p> <p>The visualization images are stored in det_rec folder, as shown in the picture.</p> <p> </p> <p> Visualization of text detection and recognition result </p> <p>The recognition results are saved in det_rec/pipeline_results.txt in the following format:</p> <pre><code>img_498.jpg [{\"transcription\": \"keep\", \"points\": [[819.0, 71.0], [888.0, 67.0], [891.0, 104.0], [822.0, 108.0]]}, {...}]\n</code></pre>"},{"location":"inference/inference_tutorial/#43-detection","title":"4.3 Detection","text":"<p>Run text detection alone.</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --res_save_dir=det \\\n    --vis_det_save_dir=det\n</code></pre> <p>The visualization results are stored in the det folder, as shown in the picture.</p> <p> </p> <p> Visualization of text detection result </p> <p>The detection results are saved in the det/det_results.txt file in the following format:</p> <pre><code>img_108.jpg [[[226.0, 442.0], [402.0, 416.0], [404.0, 433.0], [228.0, 459.0]], [...]]\n</code></pre>"},{"location":"inference/inference_tutorial/#44-classification","title":"4.4 Classification","text":"<p>Run text angle classification alone.</p> <pre><code># cls_mv3.mindir is converted from ppocr\npython infer.py \\\n    --input_images_dir=/path/to/images \\\n    --cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --res_save_dir=cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"inference/inference_tutorial/#45-recognition","title":"4.5 Recognition","text":"<p>Run text recognition alone.</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --backend=lite \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=rec\n</code></pre> <p>Note: set <code>--character_dict_path=/path/to/xxx_dict.txt</code> if not only use numbers and lowercase.</p> <p>The results will be saved in rec/rec_results.txt, with the following format:</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"inference/inference_tutorial/#46-detail-of-inference-parameter","title":"4.6 Detail of inference parameter","text":"Details   - Basic settings    | name             | type | default | description                                              |   |:-----------------|:-----|:--------|:---------------------------------------------------------|   | input_images_dir | str  | None    | Image or folder path for inference                       |   | device           | str  | Ascend  | Device type, support Ascend                              |   | device_id        | int  | 0       | Device id                                                |   | backend          | str  | lite    | Inference backend, support acl, lite                     |   | parallel_num     | int  | 1       | Number of parallel in each stage of pipeline parallelism |   | precision_mode   | str  | None    | Precision mode, only supports setting by [Model Conversion](convert_tutorial.md) currently, and it takes no effect here |  - Saving Result    | name                  | type | default           | description                                            |   |:----------------------|:-----|:------------------|:-------------------------------------------------------|   | res_save_dir          | str  | inference_results | Saving dir for inference results                       |   | vis_det_save_dir      | str  | None              | Saving dir for images of with detection boxes          |   | vis_pipeline_save_dir | str  | None              | Saving dir for images of with detection boxes and text |   | vis_font_path         | str  | None              | Font path for drawing text                             |   | crop_save_dir         | str  | None              | Saving path for cropped images after detection         |   | show_log              | bool | False             | Whether show log when inferring                        |   | save_log_dir          | str  | None              | Log saving dir                                         |  - Text detection    | name                     | type | default | description                                            |   |:-------------------------|:-----|:--------|:-------------------------------------------------------|   | det_model_path           | str  | None    | Model path for text detection                          |   | det_model_name_or_config | str  | None    | Model name or YAML config file path for text detection |  - Text angle classification    | name                     | type | default | description                                                       |   |:-------------------------|:-----|:--------|:------------------------------------------------------------------|   | cls_model_path           | str  | None    | Model path for text angle classification                          |   | cls_model_name_or_config | str  | None    | Model name or YAML config file path for text angle classification |  - Text recognition    | name                     | type | default | description                                                                 |   |:-------------------------|:-----|:--------|:----------------------------------------------------------------------------|   | rec_model_path           | str  | None    | Model path for text recognition                                             |   | rec_model_name_or_config | str  | None    | Model name or YAML config file path for text recognition                    |   | character_dict_path      | str  | None    | Dict file for text recognition\uff0cdefault only supports numbers and lowercase |  Notes\uff1a  `*_model_name_or_config` can be the model name or YAML config file path, please refer to [MindOCR models list](mindocr_models_list.md)\uff0c[PPOCR models list](thirdparty_models_list.md)."},{"location":"inference/inference_tutorial/#5-model-inference-evaluation","title":"5. Model Inference Evaluation","text":""},{"location":"inference/inference_tutorial/#51-text-detection","title":"5.1 Text detection","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_det.py \\\n    --gt_path=/path/to/det_gt.txt \\\n    --pred_path=/path/to/prediction/det_results.txt\n</code></pre>"},{"location":"inference/inference_tutorial/#52-text-recognition","title":"5.2 Text recognition","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n    --gt_path=/path/to/rec_gt.txt \\\n    --pred_path=/path/to/prediction/rec_results.txt \\\n    --character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>Please note that character_dict_path is an optional parameter, and the default dictionary only supports numbers and English lowercase.</p> <p>When evaluating the PaddleOCR series models, please refer to Third-party Model Support List to use the corresponding dictionary.</p>"},{"location":"inference/mindocr_models_list/","title":"MindOCR Models List","text":""},{"location":"inference/mindocr_models_list/#mindocr-support-models-list","title":"MindOCR Support Models List","text":"<p>Note: All results is test on 310P3 with MindSpore2.2.14.</p>"},{"location":"inference/mindocr_models_list/#text-detection","title":"Text Detection","text":"Model Backbone Language Datset F-score(%) FPS Data Shape (NCHW) Lite convert config txt Configuration File Download Link DBNet MobileNetV3 en IC15 76.96 24.90 (1,3,736,1280) config.txt yaml mindir ResNet-18 en IC15 81.73 24.16 (1,3,736,1280) config.txt yaml mindir ResNet-50 en IC15 85.00 25.98 (1,3,736,1280) config.txt yaml mindir ResNet-50 ch + en / / / (1,3,736,1280) config.txt yaml mindir DBNet++ ResNet-50 en IC15 86.79 6.75 (1,3,1152,2048) config.txt yaml mindir ResNet-50 ch + en / / / (1,3,1152,2048) config.txt yaml mindir EAST ResNet-50 en IC15 84.18 23.52 (1,3,720,1280) config.txt yaml mindir MobileNetV3 en IC15 75.08 24.37 (1,3,720,1280) config.txt yaml mindir PSENet ResNet-152 en IC15 81.69 2.94 (1,3,1472,2624) config.txt yaml mindir ResNet-50 en IC15 81.36 9.94 (1,3,736,1312) config.txt yaml mindir MobileNetV3 en IC15 70.67 10.33 (1,3,736,1312) config.txt yaml mindir FCENet ResNet50 en IC15 77.99 16.97 (1,3,736,1280) config.txt yaml mindir"},{"location":"inference/mindocr_models_list/#text-recognition","title":"Text Recognition","text":"Model Backbone Character Dict Dataset Acc(%) FPS Data Shape (NCHW) Lite convert config txt Configuration File Download Link CRNN VGG7 Default IC15 66.01 394.30 (1,3,32,100) config.txt yaml mindir ResNet34_vd Default IC15 69.67 339.45 (1,3,32,100) config.txt yaml mindir ResNet34_vd ch_dict.txt / / / (1,3,32,320) config.txt yaml mindir SVTR Tiny Default IC15 80.02 314.08 (1,3,64,256) config.txt yaml mindir Rare ResNet34_vd Default IC15 69.48 239.66 (1,3,32,100) config.txt yaml mindir ResNet34_vd ch_dict.txt / / / (1,3,32,320) config.txt yaml mindir RobustScanner ResNet-31 en_dict90.txt IC15 78.62 63.81 (1,3,48,160) config.txt yaml mindir VisionLAN ResNet-45 Default IC15 80.07 301.49 (1,3,64,256) config.txt yaml(LA) mindir(LA)"},{"location":"inference/mindocr_models_list/#text-direction-classification","title":"Text Direction Classification","text":"Model Backbone Dataset Acc(%) FPS Data Shape (NCHW) Lite convert config txt Configuration File Download Link MobileNetV3 MobileNetV3 / / / (1,3,48,192) config.txt yaml mindir"},{"location":"inference/thirdparty_models_list/","title":"Third-party Models List","text":""},{"location":"inference/thirdparty_models_list/#third-party-models-support-list","title":"Third-party Models Support List","text":"<p>MindOCR can support inference for third-party models such as PaddleOCR, etc. This document presents a list of adapted models. Performance testing is based on Ascend310P, and some models currently do not have a test dataset.</p> <p>Note: All results is test on 310P3 with MindSpore2.2.14.</p>"},{"location":"inference/thirdparty_models_list/#text-detection","title":"Text Detection","text":"Name Model Backbone Dataset F-score(%) FPS Origin link Configuration File Original Download Reference Link ONNX Data Shape (NCHW) Lite convert config txt ch_pp_det_OCRv4 DBNet MobileNetV3 / / / PaddleOCR yaml infer model ch_PP-OCRv4_det onnx (-1,3,-1,-1) config txt ch_pp_server_det_v2.0 DBNet ResNet18_vd MLT17 48.28 12.68 PaddleOCR yaml infer model ch_ppocr_server_v2.0_det onnx (1,3,736,1280) config txt ch_pp_det_OCRv3 DBNet MobileNetV3 MLT17 35.02 23.91 PaddleOCR yaml infer model ch_PP-OCRv3_det onnx (-1,3,-1,-1) config txt ch_pp_det_OCRv2 DBNet MobileNetV3 MLT17 44.80 13.00 PaddleOCR yaml infer model ch_PP-OCRv2_det onnx (1,3,736,1280) config txt ch_pp_mobile_det_v2.0_slim DBNet MobileNetV3 MLT17 33.48 12.36 PaddleOCR yaml infer model ch_ppocr_mobile_slim_v2.0_det onnx (1,3,736,1280) config txt ch_pp_mobile_det_v2.0 DBNet MobileNetV3 MLT17 33.36 12.77 PaddleOCR yaml infer model ch_ppocr_mobile_v2.0_det onnx (1,3,736,1280) config txt en_pp_det_OCRv3 DBNet MobileNetV3 IC15 43.73 40.53 PaddleOCR yaml infer model en_PP-OCRv3_det onnx (-1,3,-1,-1) config txt ml_pp_det_OCRv3 DBNet MobileNetV3 MLT17 68.79 20.12 PaddleOCR yaml infer model ml_PP-OCRv3_det onnx (-1,3,-1,-1) config txt en_pp_det_dbnet_resnet50vd DBNet ResNet50_vd IC15 79.25 23.71 PaddleOCR yaml infer model DBNet onnx (1,3,736,1280) config txt en_pp_det_psenet_resnet50vd PSE ResNet50_vd IC15 82.01 10.22 PaddleOCR yaml train model PSE onnx (1,3,736,1280) config txt en_pp_det_east_resnet50vd EAST ResNet50_vd IC15 84.80 23.32 PaddleOCR yaml train model EAST onnx (1,3,736,1280) config txt en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 86.28 17.45 PaddleOCR yaml train model SAST onnx (1,3,-1,-1) config txt <p>Notice: When using the en_pp_det_psenet_resnet50vd model for inference, you need to modify the onnx file with the following command</p> <pre><code>python deploy/models_utils/onnx_optim/insert_pse_postprocess.py \\\n      --model_path=./pse_r50vd.onnx \\\n      --binary_thresh=0.0 \\\n      --scale=1.0\n</code></pre>"},{"location":"inference/thirdparty_models_list/#text-recognition","title":"Text Recognition","text":"Name Model Backbone Dataset Acc(%) FPS Source Dict file Configuration File Original Download reference ONNX Data Shape (NCHW) Lite convert config txt ch_pp_rec_OCRv4 CRNN MobileNetV1Enhance / / 310.71 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv4_rec onnx (1,3,32,100) config txt ch_pp_server_rec_v2.0 CRNN ResNet34 / / 259.82 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_ppocr_server_v2.0_rec onnx (1,3,32,100) config txt ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 430.02 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv3_rec onnx (1,3,32,100) config txt ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance / / 321.91 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv2_rec onnx (1,3,32,100) config txt ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 / / 278.21 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_ppocr_mobile_v2.0_rec onnx (1,3,32,100) config txt en_pp_rec_OCRv3 SVTR MobileNetV1Enhance IC15 49.83 580.88 PaddleOCR en_dict.txt yaml infer model en_PP-OCRv3_rec onnx (1,3,32,100) config txt en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 / / 229.21 PaddleOCR en_dict.txt yaml infer model en_number_mobile_slim_v2.0_rec onnx (1,3,32,320) config txt en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 IC15 46.08 228.97 PaddleOCR en_dict.txt yaml infer model en_number_mobile_v2.0_rec onnx (1,3,32,320) config txt korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 511.92 PaddleOCR korean_dict.txt yaml infer model korean_PP-OCRv3_rec onnx (1,3,32,100) config txt japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 477.32 PaddleOCR japan_dict.txt yaml infer model japan_PP-OCRv3_rec onnx (1,3,32,100) config txt chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 390.15 PaddleOCR chinese_cht_dict.txt yaml infer model chinese_cht_PP-OCRv3_rec onnx (1,3,32,100) config txt te_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 582.82 PaddleOCR te_dict.txt yaml infer model te_PP-OCRv3_rec onnx (1,3,32,100) config txt ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 589.41 PaddleOCR ka_dict.txt yaml infer model ka_PP-OCRv3_rec onnx (1,3,32,100) config txt ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 587.30 PaddleOCR ta_dict.txt yaml infer model ta_PP-OCRv3_rec onnx (1,3,32,100) config txt latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 584.57 PaddleOCR latin_dict.txt yaml infer model latin_PP-OCRv3_rec onnx (1,3,32,100) config txt arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 578.28 PaddleOCR arabic_dict.txt yaml infer model arabic_PP-OCRv3_rec onnx (1,3,32,100) config txt cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 573.57 PaddleOCR cyrillic_dict.txt yaml infer model cyrillic_PP-OCRv3_rec onnx (1,3,32,100) config txt devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / 589.43 PaddleOCR devanagari_dict.txt yaml infer model devanagari_PP-OCRv3_rec onnx (1,3,32,100) config txt en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd IC15 66.35 392.97 PaddleOCR ic15_dict.txt yaml infer model CRNN onnx (1,3,32,160) config txt en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd IC15 64.52 487.68 PaddleOCR ic15_dict.txt yaml infer model Rosetta onnx (1,3,32,160) config txt en_pp_rec_vitstr_vitstr ViTSTR ViTSTR IC15 68.42 336.53 PaddleOCR EN_symbol_dict.txt yaml train model ViTSTR onnx (1,1,224,224) config txt"},{"location":"inference/thirdparty_models_list/#text-angle-classification","title":"Text Angle Classification","text":"Name Model Dataset Acc(%) FPS Source Configuration File Original Download Reference OONX Data Shape (NCHW) Lite convert config txt ch_pp_mobile_cls_v2.0 MobileNetV3 / / / PaddleOCR yaml infer model ch_ppocr_mobile_v2.0_cls onnx (1,3,48,192) config txt"},{"location":"mkdocs/contributing/","title":"MindOCR Contributing Guidelines","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little helps, and credit will always be given.</p>"},{"location":"mkdocs/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindOCR community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"mkdocs/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"mkdocs/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"mkdocs/contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"mkdocs/contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to fix it.</p>"},{"location":"mkdocs/contributing/#write-documentation","title":"Write Documentation","text":"<p>MindOCR could always use more documentation, whether as part of the official MindOCR docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"mkdocs/contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"mkdocs/contributing/#getting-started","title":"Getting Started","text":"<p>Ready to contribute? Here's how to set up <code>mindocr</code> for local development.</p> <ol> <li>Fork the <code>mindocr</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindocr.git\n</code></pre> <p>After that, you should add official repository as the upstream repository:</p> <pre><code>git remote add upstream git@github.com:mindspore-lab/mindocr\n</code></pre> <ol> <li>Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development:</li> </ol> <pre><code>conda create -n mindocr python=3.8\nconda activate mindocr\ncd mindocr\npip install -e .\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you finish making changes, check that your changes pass the linters and the tests:</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>If all static linting are passed, you will get output like:</p> <p></p> <p>otherwise, you need to fix the warnings according to the output:</p> <p></p> <p>To get pre-commit and pytest, just pip install them into your conda environment.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"mkdocs/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9, and for PyPy. Check    https://github.com/mindspore-lab/mindocr/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"mkdocs/contributing/#tips","title":"Tips","text":"<p>You can install the git hook scripts instead of linting with <code>pre-commit run -a</code> manually.</p> <p>run flowing command to set up the git hook scripts</p> <pre><code>pre-commit install\n</code></pre> <p>now <code>pre-commit</code> will run automatically on <code>git commit</code>!</p>"},{"location":"mkdocs/contributing/#releasing","title":"Releasing","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed. Then run:</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Action will then deploy to PyPI if tests pass.</p>"},{"location":"mkdocs/customize_data_transform/","title":"Customize Data Transformation","text":""},{"location":"mkdocs/customize_data_transform/#guideline-for-developing-your-transformation","title":"Guideline for Developing Your Transformation","text":""},{"location":"mkdocs/customize_data_transform/#writing-guideline","title":"Writing Guideline","text":"<ol> <li> <p>Each transformation is a class with a callable function. An example is shown below.</p> </li> <li> <p>The input to the transformation function is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>Please write comments for the call function to clarify the required/modified/added keys in the data dict.</p> </li> <li> <p>Add kwargs in the class init function for extension, which is used to parse global config, such as is_train.</p> </li> </ol> <pre><code>class ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    \"\"\"\n\n    def __init__(self, channel, **kwargs):\n        self.is_train = kwargs.get('is_train', True)\n\n    def __call__(self, data: dict):\n        '''\n        required keys:\n            - image\n        modified keys:\n            - image\n        '''\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre>"},{"location":"mkdocs/customize_data_transform/#add-unit-test-and-visualization","title":"Add Unit Test and Visualization","text":"<p>Please add unit test in <code>tests/ut/transforms</code> for the written transformation and try to cover different cases (inputs and settings).</p> <p>Please visually check the correctness of the transformation on image and annotation using the jupyter notebook. See <code>transform_tutorial.ipynb</code>.</p>"},{"location":"mkdocs/customize_data_transform/#important-notes","title":"Important Notes","text":"<ol> <li>For spatial transformation operaions that will be used in text detection inference or evaluation (e.g. determinstic resize, scale), please record the space transformation information in <code>shape_list</code>. Otherwise, the postprocessing method won't be able to map the results back to the orignal image space. On how to record <code>shape_list</code>, please refer to DetResize.</li> </ol>"},{"location":"mkdocs/customize_dataset/","title":"Customize Dataset","text":""},{"location":"mkdocs/customize_dataset/#guideline-for-data-module","title":"Guideline for Data Module","text":""},{"location":"mkdocs/customize_dataset/#code-structure","title":"Code Structure","text":"<pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base_dataset.py                 # base dataset class with __getitem__\n\u251c\u2500\u2500 builder.py                  # API for create dataset and loader\n\u251c\u2500\u2500 det_dataset.py              # general text detection dataset class\n\u251c\u2500\u2500 rec_dataset.py              # general rec detection dataset class\n\u251c\u2500\u2500 rec_lmdb_dataset.py             # LMDB dataset class\n\u2514\u2500\u2500 transforms\n    \u251c\u2500\u2500 det_transforms.py           # processing and augmentation ops (callabel classes) especially for detection tasks\n    \u251c\u2500\u2500 general_transforms.py           # general processing and augmentation ops (callabel classes)\n    \u251c\u2500\u2500 modelzoo_transforms.py          # transformations adopted from modelzoo\n    \u251c\u2500\u2500 rec_transforms.py           # processing and augmentation ops (callabel classes) especially for recognition tasks\n    \u2514\u2500\u2500 transforms_factory.py           # API for create and run transforms\n</code></pre>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-dataset-class","title":"How to add your own dataset class","text":"<ol> <li> <p>Inherit from BaseDataset class</p> </li> <li> <p>Rewrite the following file and annotation parsing functions in BaseDataset.</p> <p>def load_data_list(self, label_file: Union[str, List[str]], sample_ratio: Union[float, List] = 1.0,  shuffle: bool = False, **kwargs) -&gt; List[dict]</p> <p>def _parse_annotation(self, data_line: str) -&gt; Union[dict, List[dict]]</p> </li> </ol>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-data-transformation","title":"How to add your own data transformation","text":"<p>Please refer to Guideline for Developing Your Transformation</p>"},{"location":"mkdocs/customize_model/","title":"Guideline for Model Module","text":""},{"location":"mkdocs/customize_model/#how-to-add-a-new-model-in-mindocr","title":"How to Add a New Model in MindOCR","text":"<ol> <li> <p>Decompose the model into 3 (or 2) modules: backbone, (neck,) head. Neck is usually not involved in recognition tasks.</p> </li> <li> <p>For each module:</p> <p>a. if it is implemented in MindOCR, skip since you can get the module by the <code>build_{module}</code> function .</p> <p>b. if not, please implement it and follow the module format guideline</p> </li> <li> <p>Define your model in two ways</p> <p>a. Write a model py file, which includes the model class and specification functions. Please follow the model format guideline. It is to allows users to invoke a pre-defined model easily, such as <code>model = build_model('dbnet_resnet50', pretrained=True)</code>  .</p> <p>b. Config the architecture in a yaml file. Please follow the yaml format guideline . It is to allows users to modify a base architecture quickly in yaml file.</p> </li> <li> <p>To verify the correctness of the written model, please add your yaml config file path in <code>test_models.py</code>, modify the main function to build the desired model, and then run <code>test_models.py</code></p> </li> </ol> <pre><code>python tests/ut/test_models.py --config /path/to/yaml_config_file\n</code></pre>"},{"location":"mkdocs/customize_model/#format-guideline-for-writing-a-new-module","title":"Format Guideline for Writing a New Module","text":""},{"location":"mkdocs/customize_model/#backbone","title":"Backbone","text":"<ul> <li>File naming format: <code>models/backbones/{task}_{backbone}.py</code>, e.g, <code>det_resnet.py</code>   (since the same backbone for det and rec may differ, the task prefix is necessary)</li> <li>Class naming format: {Task}{BackboneName}{Variant} e.g. <code>class DetResNet</code></li> <li>Class <code>__init__</code> args: no limitation, define by model need.</li> <li>Class attributes: MUST contain <code>out_channels</code> (List), to describe channels of each output features. e.g. <code>self.out_channels=[256, 512, 1024, 2048]</code></li> <li>Class <code>construct</code> args: x (Tensor)</li> <li>Class <code>construct</code> return: features (List[Tensor]) for features extracted from different layers in the backbone, feature dim order <code>[bs, channels, \u2026]</code>. Expect shape of each feature: <code>[bs, channels, H, W]</code></li> </ul>"},{"location":"mkdocs/customize_model/#neck","title":"Neck","text":"<ul> <li>File naming format: <code>models/necks/{neck_name}.py</code>, e.g, <code>fpn.py</code></li> <li>Class naming format: {NeckName} e.g. <code>class FPN</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=256, **kwargs)</code>.</li> <li>Class attributes: MUST contain <code>out_channels</code> attribute, to describe channel of the output feature. e.g. <code>self.out_channels=256</code></li> <li>Class <code>construct</code> args: features (List(Tensor))</li> <li>Class <code>construct</code> return: feature (Tensor) for output feature, feature dim order <code>[bs, channels, \u2026]</code></li> </ul>"},{"location":"mkdocs/customize_model/#head","title":"Head","text":"<ul> <li>File naming: <code>models/heads/{head_name}.py</code>, e.g., <code>dbhead.py</code></li> <li>Class naming: {HeadName} e.g. <code>class DBHead</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=2, **kwargs)</code>.</li> <li>Class <code>construct</code> args: feature (Tensor), extra_input (Optional[Tensor]). The extra_input tensor is only applicable for head that needs recurrent input (e.g., Attention head), or heads with multiple inputs.</li> <li>Class <code>construct</code> return: prediction (Union(Tensor, Tuple[Tensor])). If there is only one output, return Tensor. If there are multiple outputs, return Tuple of Tensor, e.g., <code>return output1, output2, output_N</code>. Note that the order should match the loss function or the postprocess function.</li> </ul> <p>Note: if there is no neck in the model architecture like crnn, you can skip writing for neck. <code>BaseModel</code> will select the last feature of the features (List(Tensor)) output by Backbone, and forward it to Head module.</p>"},{"location":"mkdocs/customize_model/#format-guideline-for-model-py-file","title":"Format Guideline for Model Py File","text":"<ul> <li>File naming: <code>models/{task}_{model_class_name}.py</code>, e.g., <code>det_dbnet.py</code></li> <li>Class naming: {ModelName}, e.g., <code>class DBNet</code></li> <li>Class MUST inherent from <code>BaseModel</code>, e.g., <code>class DBNet(BaseModel)</code></li> <li>Spec. function naming: <code>{model_class_name}_{specifiation}.py</code>, e.g. <code>def dbnet_resnet50()</code> (Note: no need to add task prefix assuming no one model can solve any two tasks)</li> <li>Spec. function args: (pretrained=False, **kwargs), e.g. <code>def dbnet_resnet50(pretrained=False, **kwargs)</code>.</li> <li>Spec. function return: model (nn.Cell), which is the model instance</li> <li>Spec. function decorator: MUST add @register_model decorator, and import model file in <code>mindocr/models/__init__.py</code>, which is to register the model to the supported model list.</li> </ul> <p>After writing and registration, model can be created via the <code>build_model</code> func.  ``` python</p>"},{"location":"mkdocs/customize_model/#in-a-python-script","title":"in a python script","text":"<p>model = build_model('dbnet_resnet50', pretrained=False) <pre><code>## Format Guideline for Yaml File\n\nTo define/config the model architecture in yaml file, you should follow the keys in the following examples.\n\n\n- For models with a neck.\n\n``` python\nmodel:              # R\n  type: det\n  backbone:             # R\n    name: det_resnet50      # R, backbone specification function name\n    pretrained: False\n  neck:             # R\n    name: FPN           # R, neck class name\n    out_channels: 256       # D, neck class __init__ arg\n    #use_asf: True\n  head:             # R, head class name\n    name: ConvHead      # D, head class __init__ arg\n    out_channels: 2\n    k: 50\n</code></pre></p> <ul> <li>For models without a neck <pre><code>model:              # R\n  type: rec\n  backbone:         # R\n    name: resnet50      # R\n    pretrained: False\n  head:             # R\n    name: ConvHead      # R\n    out_channels: 30        # D\n</code></pre></li> </ul> <p>(R - Required. D - Depends on model)</p>"},{"location":"mkdocs/customize_postprocess/","title":"Customize Postprocessing Method","text":""},{"location":"mkdocs/customize_postprocess/#guideline-for-postprocessing-module","title":"Guideline for Postprocessing Module","text":""},{"location":"mkdocs/customize_postprocess/#common-protocols","title":"Common Protocols","text":"<ol> <li>Each postprocessing module is a class with a callable function.</li> <li>The input to the postprocessing function is network prediction and additional data information if needed.</li> <li>The output of the postprocessing function is a alwasy a dict, where the key is a field name, such as 'polys' for polygons in text detection, 'text' for text detection.</li> </ol>"},{"location":"mkdocs/customize_postprocess/#detection-postprocessing-api-protocols","title":"Detection Postprocessing API Protocols","text":"<ol> <li> <p>class naming: Det{Method}Postprocess</p> </li> <li> <p>class  <code>__init__()</code> args:</p> <ul> <li><code>box_type</code> (string): options are [\"quad', 'polys\"] for quadriateral and polygon text representation.</li> <li><code>rescale_fields</code> (List[str]='polys'): indicates which fields in the output dict will be rescaled to the original image space. Field name: \"polys\" for polygons</li> </ul> </li> <li> <p><code>__call__()</code> method: If inherit from <code>DetBasePostprocess</code>DetBasePostprocess<code>`, you don't need to implement this method in your Postproc. class.     Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get text boxes (by</code>self._postprocess()<code>function) and then rescale them back to the original image space (by</code>self.rescale()` function).</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>shape_list</code> (Union[List, np.ndarray, ms.Tensor]): shape and scale info for each image in the batch, shape [batch_size, 4]. Each internal array of length 4 is [src_h, src_w, scale_h, scale_w], where src_h and src_w are height and width of the original image, and scale_h and scale_w are their scale ratio after image resizing respectively.</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: detection result as a dictionary with the following keys</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons mapped on the original image space, shape [batch_size, num_polygons, num_points, 2]. If <code>box_type</code> is 'quad', num_points=4, and the internal np.ndarray is of shape [4, 2]</li> <li><code>scores</code> (List[float]): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> </ul> </li> <li> <p><code>_postprocess()</code> method: Implement your postprocessing method here if inherit from <code>DetBasePostprocess</code>     Postprocess network prediction to get text boxes on the transformed image space (which will be rescaled back to original image space in call function)</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: postprocessing result as a dict with keys:</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons on the transformed (i.e. resized normally) image space, of shape (batch_size, num_polygons, num_points, 2). If <code>box_type</code> is 'quad', num_points=4.</li> <li><code>scores</code> (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>Please cast <code>pred</code> to the type you need in your implementation. Some postprocesssing steps use ops from mindspore.nn and prefer Tensor type, while some steps prefer np.ndarray type required in other libraries.</li> <li><code>_postprocess()</code> should NOT round the text box <code>polys</code> to integer in return, because they will be recaled and then rounded in the end. Rounding early will cause larger error in polygon rescaling and results in evaluation performance degradation, especially on small datasets.</li> </ul> </li> </ul> </li> <li> <p>About rescaling the polygons back to the original image spcae</p> <ul> <li>The rescaling step is necessary for a fair evaluation and is needed in cropping text regions from the orginal image in inference.</li> <li>To enable rescaling for evaluation<ol> <li>add \"shape_list\" to the <code>eval.dataset.output_columns</code> in the YAML config file of the model.</li> <li>make sure <code>rescale_fields</code> is not None (default is [\"polys\"])</li> </ol> </li> <li>To enable rescaling in inference:<ol> <li>directly parse <code>shape_list</code> (which is got from data[\"shape_list\"] after data loading) to the postprocessing function.  It works with <code>rescale_fields</code> to decide whether to do rescaling and which fields are to be rescaled.</li> </ol> </li> <li><code>shape_list</code> is originally recorded in image resize transformation, such as <code>DetResize</code>.</li> </ul> </li> </ol> <p>Example Code: DetBasePostprocess and DetDBPostprocess</p>"},{"location":"mkdocs/customize_postprocess/#recognition-postprocessing-api-protocols","title":"Recognition Postprocessing API Protocols","text":"<ol> <li> <p>class  <code>__init__()</code> should support the follow args:         - character_dict_path         - use_space_char         - blank_at_last         - lower     Please see the API docs in RecCTCLabelDecode for argument illustration.</p> </li> <li> <p><code>__call__()</code> method:</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: det_res as a dictionary with the following keys</p> <ul> <li><code>texts</code> (List[str]): list of preditected text string</li> <li><code>confs</code> (List[float]): confidence of each prediction</li> </ul> </li> </ul> </li> </ol> <p>Example code: RecCTCLabelDecode</p>"},{"location":"mkdocs/license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>\u00a9 You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright yyyy</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"mkdocs/modelzoo_training/","title":"Training","text":""},{"location":"mkdocs/modelzoo_training/#training-mindocr-models-list","title":"Training - MindOCR Models List","text":""},{"location":"mkdocs/modelzoo_training/#text-detection","title":"Text Detection","text":"model dataset bs cards F-score ms/step fps amp config dbnet_mobilenetv3 icdar2015 10 1 77.28 100 100 O0 mindocr_dbnet dbnet_resnet18 icdar2015 20 1 81.73 186 108 O0 mindocr_dbnet dbnet_resnet50 icdar2015 10 1 85.05 133 75.2 O0 mindocr_dbnet dbnet++_resnet50 icdar2015 32 1 86.74 571 56 O0 mindocr_dbnet++ psenet_resnet152 icdar2015 8 8 82.06 769.6 83.16 O0 mindocr_psenet psenet_resnet50 icdar2015 8 8 81.37 304.138 210.43 O0 mindocr_psenet psenet_mobilenetv3 icdar2015 8 8 70.56 173.604 368.66 O0 mindocr_psenet east_mobilenetv3 icdar2015 20 8 75.32 138 1185 O0 mindocr_east east_resnet50 icdar2015 20 8 84.87 256 625 O0 mindocr_east fcenet_resnet50 icdar2015 8 4 84.12 2978.65 10.36 O0 mindocr_fcenet"},{"location":"mkdocs/modelzoo_training/#text-recognition","title":"Text Recognition","text":"model dataset bs cards acc ms/step fps amp config svtr_tiny IC03,13,15,IIIT,etc 512 4 90.23 459 4560 O2 mindocr_svtr crnn_vgg7 IC03,13,15,IIIT,etc 16 8 82.03 22.06 5802.71 O3 mindocr_crnn crnn_resnet34_vd IC03,13,15,IIIT,etc 64 8 84.45 76.48 6694.84 O3 mindocr_crnn rare_resnet34_vd IC03,13,15,IIIT,etc 512 4 85.19 449 4561 O2 mindocr_rare visionlan_resnet45 IC03,13,15,IIIT,etc 192 4 90.61 417 1840 O2 mindocr_visionlan master_resnet31 IC03,13,15,IIIT,etc 512 4 90.37 747 2741 O2 mindocr_master robustscanner_resnet31 IC13,15,IIIT,SVT,etc 256 4 87.86 825 310 O0 mindocr_robustscanner abinet_resnet45 IC03,13,15,IIIT,etc 768 8 91.35 718 628.11 O0 mindocr_abinet"},{"location":"mkdocs/modelzoo_training/#text-direction-classification","title":"Text Direction Classification","text":"model dataset bs cards acc ms/step fps amp config mobilenetv3 RCTW17,MTWI,LSVT 256 4 94.59 172.9 5923.5 O0 mindocr_mobilenetv3"},{"location":"mkdocs/online_inference/","title":"MindOCR Online Inference","text":"<p>About Online Inference: Online inference is to infer based on the native MindSpore framework by loading the model checkpoint file then running prediction with MindSpore APIs.</p> <p>Compared to offline inference (which is implemented in <code>deploy/py_infer</code> in MindOCR), online inferece does not require model conversion for target platforms and can run directly on the training devices (e.g. Ascend 910). But it requires installing the heavy AI framework and the model is not optimized for deployment.</p> <p>Thus, online inference is more suitable for demonstration and to visually evaluate model generalization ability on unseen data.</p>"},{"location":"mkdocs/online_inference/#dependency-and-installation","title":"Dependency and Installation","text":"<p>To be consistent with training environment.</p>"},{"location":"mkdocs/online_inference/#text-detection","title":"Text Detection","text":"<p>To run text detection on an input image or a directory containing multiple images, please execute</p> <pre><code>python tools/infer/text/predict_det.py  --image_dir {path_to_img or dir_to_imgs} --det_algorithm DB++\n</code></pre> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/det_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection result on img_108.jpg </p> <p>, where the saved txt file is as follows <pre><code>img_108.jpg [[[228, 440], [403, 413], [406, 433], [231, 459]], [[282, 280], [493, 252], [499, 293], [288, 321]], [[500, 253], [636, 232], [641, 269], [505, 289]], ...]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p> Visualization of text detection result on paper_sam.png </p> <p>, where the saved txt file is as follows <pre><code>paper_sam.png   [[[1161, 340], [1277, 340], [1277, 378], [1161, 378]], [[895, 335], [1152, 340], [1152, 382], [894, 378]], ...]\n</code></pre></p> <p>Notes: - For input images with high resolution, please set <code>--det_limit_side_len</code> larger, e.g., 1280. <code>--det_limit_type</code> can be set as \"min\" or \"max\", where \"min \" means limiting the image size to be at least  <code>--det_limit_side_len</code>, \"max\" means limiting the image size to be at most <code>--det_limit_side_len</code>.</p> <ul> <li> <p>For more argument illustrations and usage, please run <code>python tools/infer/text/predict_det.py -h</code> or view <code>tools/infer/text/config.py</code></p> </li> <li> <p>Currently, this script runs serially to avoid dynamic shape issue and achieve better performance.</p> </li> </ul>"},{"location":"mkdocs/online_inference/#supported-detection-algorithms-and-networks","title":"Supported Detection Algorithms and Networks","text":"<p> Algorithm Name Network Name Language DB dbnet_resnet50 English DB++ dbnetpp_resnet50 English DB_MV3 dbnet_mobilenetv3 English PSE psenet_resnet152 English <p></p> <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_det.py</code>.</p>"},{"location":"mkdocs/online_inference/#text-recognition","title":"Text Recognition","text":"<p>To run text recognition on an input image or a directory containing multiple images, please execute</p> <p><pre><code>python tools/infer/text/predict_rec.py  --image_dir {path_to_img or dir_to_imgs} --rec_algorithm CRNN\n</code></pre> After running, the inference results will be saved in <code>{args.draw_img_save_dir}/rec_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <ul> <li>English text recognition</li> </ul> <p> </p> <p>  word_1216.png  </p> <p> </p> <p>  word_1217.png  </p> <p>Recognition results: <pre><code>word_1216.png   coffee\nword_1217.png   club\n</code></pre></p> <ul> <li>Chinese text recognition:</li> </ul> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>Recognition results: <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre></p> <p>Notes: - For more argument illustrations and usage, please run <code>python tools/infer/text/predict_rec.py -h</code> or view <code>tools/infer/text/config.py</code> - Both batch-wise and single-mode inference are supported. Batch mode is enabled by default for better speed. You can set the batch size via <code>--rec_batch_size</code>. You can also run in single-mode by set <code>--det_batch_mode</code> False, which may improve accuracy if the text length varies a lot.</p>"},{"location":"mkdocs/online_inference/#supported-recognition-algorithms-and-networks","title":"Supported Recognition Algorithms and Networks","text":"<p> Algorithm Name Network Name Language CRNN crnn_resnet34 English RARE rare_resnet34 English SVTR svtr_tiny English CRNN_CH crnn_resnet34_ch Chinese RARE_CH rare_resnet34_ch Chinese <p></p> <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_rec.py</code></p> <p>Currently, space char recognition is not supported for the listed models. We will support it soon.</p>"},{"location":"mkdocs/online_inference/#text-detection-and-recognition-concatenation","title":"Text Detection and Recognition Concatenation","text":"<p>To run text spoting (i.e., detect all text regions then recognize each of them) on an input image or multiple images in a directory, please run:</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n                                          --det_algorithm DB++  \\\n                                          --rec_algorithm CRNN\n</code></pre> <p>Note: set <code>--visualize_output True</code> if you want to visualize the detection and recognition results on the input image.</p> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>,  where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection and recognition result on img_10.jpg  </p> <p>, where the saved txt file is as follows <pre><code>img_10.jpg  [{\"transcription\": \"residential\", \"points\": [[43, 88], [149, 78], [151, 101], [44, 111]]}, {\"transcription\": \"areas\", \"points\": [[152, 83], [201, 81], [202, 98], [153, 100]]}, {\"transcription\": \"when\", \"points\": [[36, 56], [101, 56], [101, 78], [36, 78]]}, {\"transcription\": \"you\", \"points\": [[99, 54], [143, 52], [144, 78], [100, 80]]}, {\"transcription\": \"pass\", \"points\": [[140, 54], [186, 50], [188, 74], [142, 78]]}, {\"transcription\": \"by\", \"points\": [[182, 52], [208, 52], [208, 75], [182, 75]]}, {\"transcription\": \"volume\", \"points\": [[199, 30], [254, 30], [254, 46], [199, 46]]}, {\"transcription\": \"your\", \"points\": [[164, 28], [203, 28], [203, 46], [164, 46]]}, {\"transcription\": \"lower\", \"points\": [[109, 25], [162, 25], [162, 46], [109, 46]]}, {\"transcription\": \"please\", \"points\": [[31, 18], [109, 20], [108, 48], [30, 46]]}]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p>  Visualization of text detection and recognition result on web_cvpr.png  </p> <p>, where the saved txt file is as follows</p> <pre><code>web_cvpr.png    [{\"transcription\": \"canada\", \"points\": [[430, 148], [540, 148], [540, 171], [430, 171]]}, {\"transcription\": \"vancouver\", \"points\": [[263, 148], [420, 148], [420, 171], [263, 171]]}, {\"transcription\": \"cvpr\", \"points\": [[32, 69], [251, 63], [254, 174], [35, 180]]}, {\"transcription\": \"2023\", \"points\": [[194, 44], [256, 45], [255, 72], [194, 70]]}, {\"transcription\": \"june\", \"points\": [[36, 45], [110, 44], [110, 70], [37, 71]]}, {\"transcription\": \"1822\", \"points\": [[114, 43], [190, 45], [190, 70], [113, 69]]}]\n</code></pre> <p>Notes: 1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_system.py -h</code> or view <code>tools/infer/text/config.py</code></p>"},{"location":"mkdocs/online_inference/#evaluation-of-the-inference-results","title":"Evaluation of the Inference Results","text":"<p>To infer on the whole ICDAR15 test set, please run: <pre><code>python tools/infer/text/predict_system.py --image_dir /path/to/icdar15/det/test_images  /\n                                          --det_algorithm {DET_ALGO}    /\n                                          --rec_algorithm {REC_ALGO}  /\n                                          --det_limit_type min  /\n                                          --det_limit_side_len 720\n</code></pre></p> <p>Note: Here we set<code>det_limit_type</code> as <code>min</code> for better performance, due to the input image in ICDAR15 is of high resolution (720x1280).</p> <p>After running, the results including image names, bounding boxes (<code>points</code>) and recognized texts (<code>transcription</code>) will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>. The format of prediction results is shown as follows.</p> <pre><code>img_1.jpg   [{\"transcription\": \"hello\", \"points\": [600, 150, 715, 157, 714, 177, 599, 170]}, {\"transcription\": \"world\", \"points\": [622, 126, 695, 129, 694, 154, 621, 151]}, ...]\nimg_2.jpg   [{\"transcription\": \"apple\", \"points\": [553, 338, 706, 318, 709, 342, 556, 362]}, ...]\n   ...\n</code></pre> <p>Prepare the ground truth file (in the same format as above), which can be obtained from the dataset conversion script in <code>tools/dataset_converters</code>, and run the following command to evaluate the prediction results.</p> <pre><code>python deploy/eval_utils/eval_pipeline.py --gt_path path/to/gt.txt --pred_path path/to/system_results.txt\n</code></pre> <p>Evaluation of the text spotting inference results on Ascend 910 with MindSpore 2.0rc1 are shown as follows.</p> <p> Det. Algorithm Rec. Algorithm Dataset Accuracy(%) FPS (imgs/s) DBNet CRNN ICDAR15 57.82 4.86 PSENet CRNN ICDAR15 47.91 1.65 PSENet (det_limit_side_len=1472 ) CRNN ICDAR15 55.51 0.44 DBNet++ RARE ICDAR15 59.17 3.47 DBNet++ SVTR ICDAR15 64.42 2.49 <p></p> <p>Notes: 1. Currently, online inference pipeline is not optimized for efficiency, thus FPS is only for comparison between models. If FPS is your highest priority, please refer to Inference on Ascend 310, which is much faster. 2. Unless extra inidication, all experiments are run with <code>--det_limit_type</code>=\"min\" and <code>--det_limit_side</code>=720. 3. SVTR is run in mixed precision mode (amp_level=O2) since it is optimized for O2.</p>"},{"location":"mkdocs/online_inference/#argument-list","title":"Argument List","text":"<p>All CLI argument definition can be viewed via <code>python tools/infer/text/predict_system.py -h</code> or reading <code>tools/infer/text/config.py</code>.</p>"},{"location":"mkdocs/online_inference/#developer-guide-how-to-add-a-new-model-for-inference","title":"Developer Guide - How to Add a New Model for Inference","text":""},{"location":"mkdocs/online_inference/#preprocessing","title":"Preprocessing","text":"<p>The optimal preprocessing strategy can vary from model to model, especially for the resize setting (keep_ratio, padding, etc). We define the preprocessing pipeline for each model in <code>tools/infer/text/preprocess.py</code> for different tasks.</p> <p>If you find the default preprocessing pipeline or hyper-params does not meet the network requirement, please extend by changing the if-else conditions or adding a new key-value pair to the <code>optimal_hparam</code> dict in <code>tools/infer/text/preprocess.py</code>, where key is the algorithm name and the value is the suitable hyper-param setting for the target network inference.</p>"},{"location":"mkdocs/online_inference/#network-inference","title":"Network Inference","text":"<p>Supported alogirhtms and their corresponding network names (which can be checked by using the <code>list_model()</code> API) are defined in the <code>algo_to_model_name</code> dict in <code>predict_det.py</code> and <code>predict_rec.py</code>.</p> <p>To add a new detection model for inference, please add a new key-value pair to <code>algo_to_model_name</code> dict, where the key is an algorithm name and the value is the corresponding network name registered in <code>mindocr/models/{your_model}.py</code>.</p> <p>By default, model weights will be loaded from the pro-defined URL in <code>mindocr/models/{your_model}.py</code>. If you want to load a local checkpoint instead, please set <code>--det_model_dir</code> or <code>--rec_model_dir</code> to the path of your local checkpoint or the directory containing a model checkpoint.</p>"},{"location":"mkdocs/online_inference/#postproprocess","title":"Postproprocess","text":"<p>Similar to preprocessing, the postprocessing method for each algorithm can vary. The postprocessing method for each algorithm is defined in <code>tools/infer/text/postprocess.py</code>.</p> <p>If you find the default postprocessing method or hyper-params does not meet the model need, please extend the if-else conditions or add a new key-value pair  to the <code>optimal_hparam</code> dict in <code>tools/infer/text/postprocess.py</code>, where the key is an algorithm name and the value is the hyper-param setting.</p>"},{"location":"reference/api_doc/","title":"Api doc","text":"<p>coming soon...</p>"},{"location":"tutorials/advanced_train/","title":"Advanced Training","text":""},{"location":"tutorials/advanced_train/#tricks-gradient-accumulation-gradient-clipping-and-ema","title":"Tricks: Gradient Accumulation, Gradient Clipping, and EMA","text":"<p>All the training tricks can be configured in the model config files. After setting, please run <code>tools/train.py</code> script to initiate training.</p> <p>Example Yaml Config</p> <pre><code>train:\n  gradient_accumulation_steps: 2\n  clip_grad: True\n  clip_norm: 5.0\n  ema: True\n  ema_decay: 0.9999\n</code></pre>"},{"location":"tutorials/advanced_train/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Gradient accumulation is an effective way to address  memory limitation issue and allow training with large global batch size.</p> <p>To enable it, set <code>train.gradient_accumulation_steps</code> to values larger than 1 in yaml config.</p> <p>The equivalent global batch size would be <code>global_batch_size = batch_size * num_devices * gradient_accumulation_steps</code></p>"},{"location":"tutorials/advanced_train/#gradient-clipping","title":"Gradient Clipping","text":"<p>Gradient clipping is a method to address gradient explosion/overflow problem and stabilize model convergence.</p> <p>To enable it, set <code>train.ema</code> to <code>True</code> and optionally adjust the norm value in <code>train.clip_norm</code>.</p>"},{"location":"tutorials/advanced_train/#ema","title":"EMA","text":"<p>Exponential Moving Average (EMA) can be viewed as a model ensemble method that smooths the model weights. It can help stabilize model convergence in training and usually leads to better model performance.</p> <p>To enable it, set <code>train.ema</code> to <code>True</code>. You may also adjust <code>train.ema_decay</code> to control the decay rate.</p>"},{"location":"tutorials/advanced_train/#resume-training","title":"Resume Training","text":"<p>Resuming training is useful when the training was interrupted unexpectedly.</p> <p>To resume training, set <code>model.resume</code> to <code>True</code> in the yaml config as follows: <pre><code>model:\n  resume: True\n</code></pre></p> <p>By default, it will resume from the \"train_resume.ckpt\" checkpoint file located in the directory specified in <code>train.ckpt_save_dir</code>.</p> <p>If you want to use another checkpoint to resume from, specify the checkpoint path in <code>resume</code> as follows:</p> <pre><code>model:\n  resume: /some/path/to/train_resume.ckpt\n</code></pre>"},{"location":"tutorials/advanced_train/#training-on-openi-cloud-platform","title":"Training on OpenI Cloud Platform","text":"<p>Please refer to the MindOCR OpenI Training Guideline</p>"},{"location":"tutorials/distribute_train/","title":"Distributed parallel training","text":"<p>This document provides a tutorial on distributed parallel training. There are two ways to train on the Ascend AI processor: by running scripts with OpenMPI or configuring <code>RANK_TABLE_FILE</code> for training. On GPU processors, scripts can be run with OpenMPI for training.</p> <p>Please ensure that the <code>distribute</code> parameter in the yaml file is set to <code>True</code> before running the following commands for distributed training.</p> <ul> <li>Distributed parallel training</li> <li>1. Ascend<ul> <li>1.1 Run scripts with OpenMPI</li> <li>1.2 Configure RANK_TABLE_FILE for training</li> <li>1.2.1 Running on Eight (All) Devices</li> <li>1.2.2 Running on Four (Partial) Devices</li> </ul> </li> <li>2. GPU<ul> <li>2.1 Run scripts with OpenMPI</li> </ul> </li> </ul>"},{"location":"tutorials/distribute_train/#1-ascend","title":"1. Ascend","text":"<p>Notes:</p> <p>On Ascend platform, some common restrictions on using the distributed service are as follows:</p> <ul> <li> <p>In a single-node system, a cluster of 1, 2, 4, or 8 devices is supported. In a multi-node system, a cluster of 8 x N devices is supported.</p> </li> <li> <p>Each host has four devices numbered 0 to 3 and four devices numbered 4 to 7 deployed on two different networks. During training of 2 or 4 devices, the devices must be connected and clusters cannot be created across networks. This means, when training with 4 devices, only <code>{0, 1, 2, 3}</code> and  <code>{4, 5, 6, 7}</code> are available. While in training with 2 devices, devices cross networks, such as <code>{0, 4}</code> are not allowed. However, devices within networks, such as <code>{0, 1}</code>or <code>{1, 2}</code>, are allowed.</p> </li> </ul>"},{"location":"tutorials/distribute_train/#11-run-scripts-with-openmpi","title":"1.1 Run scripts with OpenMPI","text":"<p>On Ascend hardware platform, users can use OpenMPI's <code>mpirun</code> to run distributed training with <code>n</code> devices. For example, in DBNet Readme, the following command is used to train the model on devices <code>0</code> and <code>1</code>:</p> <pre><code># n is the number of NPUs used in training\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>Note that <code>mpirun</code> will run training on sequential devices starting from device <code>0</code>. For example, <code>mpirun -n 4 python-command</code> will run training on the four devices: <code>{0, 1, 2, 3}</code>.</p>"},{"location":"tutorials/distribute_train/#12-configure-rank_table_file-for-training","title":"1.2 Configure RANK_TABLE_FILE for training","text":""},{"location":"tutorials/distribute_train/#121-running-on-eight-all-devices","title":"1.2.1 Running on Eight (All) Devices","text":"<p>Before using this method for distributed training, it is necessary to create an HCCL configuration file in json format, i.e. generate RANK_TABLE_FILE. The following is the command to generate the corresponding configuration file for 8 devices (for more information please refer to HCCL tools):</p> <p><pre><code>python hccl_tools.py --device_num \"[0,8)\"\n</code></pre> This command produces the following output file: <pre><code>hccl_8p_10234567_127.0.0.1.json\n</code></pre></p> <p>An example of the content in <code>hccl_8p_10234567_127.0.0.1.json</code>:</p> <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"0\",\n                    \"device_ip\": \"192.168.100.101\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"1\",\n                    \"device_ip\": \"192.168.101.101\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"2\",\n                    \"device_ip\": \"192.168.102.101\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"3\",\n                    \"device_ip\": \"192.168.103.101\",\n                    \"rank_id\": \"3\"\n                },\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"4\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"5\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"6\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"7\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> <p>Then start the training by running the following command:</p> <pre><code>bash ascend8p.sh\n</code></pre> <p>Please ensure that the <code>distribute</code> parameter in the yaml file is set to <code>True</code> before running the command.</p> <p>Here is an example of the <code>ascend8p.sh</code> script for CRNN training:</p> <p><pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=8\nexport RANK_TABLE_FILE=\"./hccl_8p_01234567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${RANK_SIZE}; i++)); do\n    export DEVICE_ID=$i\n    export RANK_ID=$i\n    echo \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\n    if [ $i -eq 0 ]; then\n      echo 'i am 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\n    else\n      echo 'not 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\n    fi\ndone\n</code></pre> When training other models, simply replace the yaml config file path in the script, i.e. <code>path/to/model_config.yaml</code>.</p> <p>After the training has started, and you can find the training log <code>train.log</code> in the project root directory.</p>"},{"location":"tutorials/distribute_train/#122-running-on-four-partial-devices","title":"1.2.2 Running on Four (Partial) Devices","text":"<p>To run training on four devices, for example, <code>{4, 5, 6, 7}</code>, the <code>RANK_TABLE_FILE</code> and the run script are different from those for running on eight devices.</p> <p>The <code>rank_table.json</code> is created by running the following command:</p> <pre><code>python hccl_tools.py --device_num \"[4,8)\"\n</code></pre> <p>This command produces the following output file: <pre><code>hccl_4p_4567_127.0.0.1.json\n</code></pre></p> <p>An example of the content in <code>hccl_4p_4567_127.0.0.1.json</code>:</p> <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"3\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> <p>Then start the training by running the following command:</p> <pre><code>bash ascend4p.sh\n</code></pre> <p>Here is an example of the <code>ascend4p.sh</code> script for CRNN training:</p> <pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=4\nexport RANK_TABLE_FILE=\"./hccl_4p_4567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${RANK_SIZE}; i++)); do\n    export DEVICE_ID=$((i+4))\n    export RANK_ID=$i\n    echo \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\n    if [ $i -eq 0 ]; then\n      echo 'i am 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\n    else\n      echo 'not 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\n    fi\ndone\n</code></pre> <p>Note that the <code>DEVICE_ID</code> and <code>RANK_ID</code> should be matched with <code>hccl_4p_4567_127.0.0.1.json</code>.</p>"},{"location":"tutorials/distribute_train/#2-gpu","title":"2. GPU","text":""},{"location":"tutorials/distribute_train/#21-run-scripts-with-openmpi","title":"2.1 Run scripts with OpenMPI","text":"<p>On GPU hardware platform, only OpenMPI's <code>mpirun</code> can be used for distributed training. The following command will run training on devices <code>0</code> and <code>1</code>.</p> <pre><code># n is the number of GPUs used in training\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>In the case when users want to run training on <code>device 2</code> and <code>device 3</code>, users can run <code>export CUDA_VISIBLE_DEVICES=2,3</code> before running the command above, or run the following command:</p> <pre><code># n is the number of GPUs used in training\nCUDA_VISIBLE_DEVICES=2,3 mpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre>"},{"location":"tutorials/frequently_asked_questions/","title":"FAQ","text":""},{"location":"tutorials/frequently_asked_questions/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<ul> <li>Undefined symbol</li> <li>Ascend so Not Found</li> <li>Ascend Error Message <code>A39999</code></li> <li><code>acl open device 0 failed</code></li> <li>Fail to install mindocr dependency on windows</li> <li><code>RunTimeError:The device address tpe is wrong</code></li> <li>Problems related to model converting</li> <li>Problems related to inference</li> <li>Training speed of DBNet not as fast as expexted</li> <li>Error about <code>libgomp-d22c30c5.so.1.0.0</code></li> <li>Dataset Pipeline Error when training abinet on lmdb dataset</li> <li>Runtime Error when training dbnet on synthtext dataset</li> <li>Failed to install seqeval</li> <li>Failed to install lanms <p>dca11cc9989deabe86985f0729502266e5ba6f42</p> </li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q1-undefined-symbol","title":"Q1 Undefined symbol","text":"<ul> <li><code>undefined symbol:_ZN9mindspore5tracel15GetDebugInfostrERKSt10shared_ptrINS_9DebugInfoEERKSsNS_13SourceLineTipE</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more infommation.\n&gt;&gt;&gt; import mindspore\n&gt;&gt;&gt; import mindspore lite\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore_lite/_init_.py\", line 26, in &lt;module&gt;\n      from mindspore lite.context import Context\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore_lite/context.py\", line 22, in &lt;module&gt;\n      from mindspore lite.lib import-c lite wrapper\nImportError: xxxx/mindspore-lite-2.2.0-linux-x64/tools/converter/lib/libmindspore_converter.so: undefined symbol:_ZN9mindspore5tracel15GetDebugInfostrERKSt10shared_ptrINS_9DebugInfoEERKSsNS_13SourceLineTipE\n</code></pre> <ul> <li><code>undefined symbol: _ZN9mindspore12label_manage23GetGlobalTraceLabelTypeEv</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\". \"credits\" or \"license\" for more infommation.\n&gt;&gt;&gt; import mindspore_lite\n&gt;&gt;&gt; import mindspore\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/ root/miniconda3/envs/xxx/1ib/python3.7/site-packages/mindspore/_init_.py\", line 18, in &lt;module&gt;\n      from mindspore.run check import run check\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore/ run_check/_init_.py\", line 17, in &lt;module&gt;\n      from . check_version import check_version_and_env_config\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspo re/run_check/check version.py\", line 29, in &lt;module&gt;\n      from mindspore._c_expression \"import MSContext, ms_ctx_param\nImportError: xxxx/mindspore-lite-2.2.0-linux-x64/tools/converter/lib/libmindspore_converter.so: undefined symbol: _ZN9mindspore12label_manage23GetGlobalTraceLabelTypeEv\n</code></pre> <ul> <li><code>undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv</code></li> </ul> <pre><code>[WARNING] LITE(20788, 7f897f04ff40, converter_lite) :2023-10-19-07:24: 10.858.973 [mindspore/lite/tools/opt imizer/common/fommat_utils.cc:385] ConvertAbstractFommatShape] abstract must be a tensor, but got: ValueAny.\n[WARNING] LITE(20788,7f897f04ff40, converter_lite) :2023-10-19-07:24: 10.858.998 [mindspore/lite/tools/optimizer/common/gllo_utils.cc: 1071] GenTransposeNode] Convertabstract failed for node: args0_nh2nc\n[WARNING] LITE(20788,7f897fO04ff40, converter_lite) :2023-10-19-07:24: 11.035.069 [mindspore/lite/src/extendrt/cxx_api/dlutils.h:124] DLSopen] dlopen /xxx/mindspore/to0ls/converter/lib/libascend pass plugin.so failed, error: /xxx/mindspore/tools/converter/1ib/libmslite_shared lib.s0: undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv\n\n[ERROR] LITE(20788,7f897f04ff40, converter_lite) :2023-10-19-07:24: 11.035.121 [mindspore/lite/tools/converter/adapter/acl/plugin/acl_pass_plugin.cc:86] CreateAclPassInner] DLSopen failed, so path: /xxx/mindspore-1ite-2.2.0.20231019-1inux-x64/tools/converter/lib/1ibascend_pass_plugin.so, ret: dlopen /xxx/mindspore/tools/converter/lib/libascend_pass_plugin.so failed, error: /xxx/mindspore/tools/converter/lib/libmslite shared lib.so: undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv\n</code></pre> <p>The problems occur due to the mismatch of <code>mindspore</code> python whl package, <code>mindspore_lite</code> python whl package and<code>mindspore_lite</code> tar package. Please check the following items according MindSpore download and MindSpore Lite download:</p> <ul> <li>Consistent version of <code>mindspore</code>, <code>mindspore_lite</code>, e.g. 2.2.0</li> <li>Consistent version of <code>mindspore_lite</code> whl and <code>mindspore_lite</code> tar package, e.g. 2.2.0</li> <li>Both <code>mindspore_lite</code> whl package and <code>mindspore_lite</code> tar package are Cloud-side</li> </ul> <p>For example, the following combination of packages is suitable when the platform is linux x86_64 with Ascend</p> <ul> <li><code>mindspore_lite whl</code>: mindspore_lite-2.2.0-cp37-cp37m-linux_x86_64.whl</li> <li><code>mindspore_lite tar.gz</code>: mindspore-lite-2.2.0-linux-x64.tar.gz</li> <li><code>mindspore whl</code>: mindspore-2.2.0-cp37-cp37m-linux_x86_64.whl</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q2-ascend-so-not-found","title":"Q2 Ascend so Not Found","text":"<ul> <li><code>dlopen mindspore_lite/lib/libascend_kernel_plugin.so</code>, No such file or directory</li> </ul> <pre><code>File \"/home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/model.py\", line 95, in warpper\n  return func(*args, **kwargs)\nFile \"/home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n  raise RuntimeError(f\"build from_file failed! Error is {ret.Tostring()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n[WARNING] ME(15411,7f07f56be100, python) : 2023-10-16-00:51:42.509.780 [mindspore/lite/src/extend rt/cxx_api/dlutils.h:124] DLSopen]\ndlopen /home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_kernel_plugin.so failed, error: libacl_cblas.so: cannot open shared object file: No such file or directory\n[ERROR] ME(15411,7f07f56be100, python) :2023-10-16-00:51:42.509.877 [mindspo re/lite/src/extendrt/kernel/ascend/plugin/ascend_allocator_plugin.cc:70] Register] DLSopen failed, so path: /home/xxx/miniconda3/envs/ yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_kernel_plugin.so , func name: CreateAclAllocator. err: dlopen /home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_ kernel_plugin.so failed, error: libacl_cblas.so: cannot open shared object file: No such file or directory\n[ERROR] ME(15411,7f07f56be100, python):2023-10-16-00:51:42.509.893 [mindspore/lite/src/extendrt/infer_session.cc:66] HandleContext] failed register ascend allocator plugin.\n...\nraise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n</code></pre> <p>This problem occur when <code>libascend_kernel plugin.so</code> is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>), which contained in <code>mindspore_lite</code> tar package. The solution is as follows:</p> <ol> <li> <p>Check <code>mindspore_lite</code> tar package has been installed. If not, please follow the MindSpore Lite download, install tar package and whl package. Please make sure the packages are Clound-side and support Ascend platform. See MindSpore download for details</p> </li> <li> <p>Find and move to the <code>mindspore_lite</code> installation path, like <code>/your_path_to/mindspore-lite</code></p> </li> <li> <p>Run command <code>find ./ -name libascend_kernel_plugin.so</code> to find the so file, and you will get the following path:</p> </li> </ol> <pre><code>./runtime/lib/libascend_kernel_plugin.so\n</code></pre> <ol> <li>Add the path to to environment variables:</li> </ol> <pre><code>export LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LD_LIBRARY_PATH\n</code></pre> <ul> <li><code>Load dynamic library: libmindspore_ascend.so.2 failed. liboptiling.so: cannot open shared object file: No such file or directory</code></li> </ul> <pre><code>python -c \"import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()\"\n[WARNING] ME(60105:13981374421 1776, MainProcess):2023-10-25-08: 14:33.640.411 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\nTraceback (most recent call last):\nFile \"&lt;string&gt;\", line 1, in module&gt;\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/_checkparam.py\", line 1313, in wrapper\n  return func(*args, **kwargs)\nFile \"/xxx/py37/1ib/python3.7/site-packages/mindspore/context.py\", line 1456, in set_context\n  ctx.set_device_target(kwargs['device target'])\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/context.py\", line 381, in set_device_target\n  self.set_param(ms_ctx_param.device_target, target)\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/context.py\", line 175, in set_param\n  self._context_handle.set_param(param, value)\nRuntimeError: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly. and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. liboptiling.so: cannot open shared object file: No such file or directory\nLoad dynamic library: 1ibmindspore ascend.so.1 failed. liboptiling.so: cannot open shared object file: No such file or directory\n----------------------------------------------------\n...\n</code></pre> <p>This problem occur when <code>liboptiling.so</code> is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>). The solution is as follows:</p> <ol> <li> <p>Check <code>CANN</code> tar package has been installed. If not, please follow the Installing Ascend AI processor software package and install CANN</p> </li> <li> <p>Find and move to the <code>CANN</code> installation path, like <code>/your_path_to/cann</code></p> </li> <li> <p>Run command <code>find ./ -name liboptiling.so</code> to find the so file, and you will get the following path:</p> <pre><code>./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/x86_64/liboptiling.so\n./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64/liboptiling.so\n./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/minios/aarch64/liboptiling.so\n</code></pre> </li> <li> <p>Add the path to to environment variables(e.g. x86_64 system):</p> <pre><code>export LD_LIBRARY_PATH=$ASCEND_HOME/CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/x86_64/:$LD_LIBRARY_PATH\n</code></pre> <p>You will find the following message if the problem is solved:</p> <pre><code>The result of multiplication calculation is correct. MindSpore has been installed on platform [Ascend] successfully!\n</code></pre> </li> </ol> <ul> <li><code>Load dynamic library: libmindspore_ascend.so.2 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory</code></li> </ul> <pre><code>RuntimeError: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly. and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_ascend.so.1 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory\n\n----------------------------------------------------\n...\n</code></pre> <p>This problem occur when <code>libaicpu_ascend_engine.so</code> is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>). The solution is as follows:</p> <ol> <li>Check <code>CANN</code> tar package has been installed. If not, please follow the Installing Ascend AI processor software package and install CANN</li> <li>Find and move to the <code>CANN</code> installation path, like <code>/your_path_to/cann</code></li> <li>Run command <code>find ./ -name libaicpu_ascend_engine.so</code> to find the so file, and you will get the following path:</li> </ol> <pre><code>./CANN-7.0/x86_64-linux/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n./CANN-7.0/compiler/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n./latest/x86_64-linux/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n</code></pre> <ol> <li> <p>Add the path to to environment variables(e.g. x86_64 system):</p> <pre><code>export LD_LIBRARY_PATH=$ASCEND_HOME/CANN-7.0/compiler/lib64/plugin/opskernel/:$LD_LIBRARY_PATH\n</code></pre> </li> </ol>"},{"location":"tutorials/frequently_asked_questions/#q3-ascend-error-message-a39999","title":"Q3 Ascend Error Message A39999","text":"<ul> <li>Error 1</li> </ul> <pre><code>----------------------------------------------------\n- Ascend Error Message:\n----------------------------------------------------\nE39999: Inner Error!\nE39999 TsdOpen failed. devId=0, tdt error=31[FUNC:PrintfTsdError] [FILE: runtime.cc][LINE:2060]\n     TraceBack (most recent call last):\n     Start aicpu executor failed, retCode=0x7020009 devId=0[FUNC :DeviceRetain][FILE: runtime.cc][LINE:2698]\n     check param failed, dev can not be NULL![FUNC:PrimaryContextRetain][FILE: runtime.cc][LINE:2544]\n     Check param failed, ctx can not be NULL! [FUNC:PrimaryContextRetain][FILE: runtime.cc][LINE:2571]\n     Check param failed, context can not be null.[FUNC:NewDevice][FILE:api impl.cc][LINE:1899]\n     New device failed, retcode=0x70 10006[FUNC:SetDevice][FILE:api_impL-cc][LINE:1922]\n     rtsetDevice execute failed, reason=[device retain error][FUNC:FuncErrorReason][FILE :error message manage.ccl[LINE:50]\n     open device 0 failed runtime result = 507033.[FUNC: ReportCallError][FILE:log_inner.cpp][LINE:161]\n\n(Please search \"Ascend Error Message\" at https://www.mindspore.cn for error code description)\n</code></pre> <ul> <li>Error 2</li> </ul> <pre><code>----------------------------------------------------\n- Ascend Error Message:\n----------------------------------------------------\nE39999: Inner Error!\nE39999 tsd client wait response fail, device response code[1]. load aicpu ops package failed, device[O], host pid[5653], error stack:\n[TSDaemon] checksum aicpu package failed, ret=103, [tsd_common.cpp:2242:SaveProcessConfig]17580\nCheck head tag failed, ret=279, [package_worker.cpp:537:VerifyAicpuPackage]2369\nVerify Aicpu package failed, srcPathlIhome/HMHiAiuser/aicpu_kernels/vf0_5653_Ascend310P-aicpu_syskernels.tar.gz]..[package_worker.cpp:567:DecompressionAicpuPackage]2369\nDecompression AicpuPackage [/home/HwHiAiUser/aicpu_kernels/vf0_5653_Ascend310P-aicpu_syskernels.tar.gz] failed, [package_worker.cpp:218:LoadAICPUPackageForProcessMode]2369\nLoad aicpu package path[/home/HwHiAiUser/hdcd/device0/] fileName[ 5653_Ascend310P-aicpu_syskernels.tar.gz] failed, [inotify_watcher.cpp:311:HandleEvent]2369\n[TSDaemon] load aicpu ops package failed, device[0], host pid[5653], [tsd_common.cpp:2054:CheckAndHandleTimeout]2374\n[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:270]\n  TraceBack (most recent call last):\n  TsdOpen failed. devId=0, tdt error=31[FUNC:PrintfTsdError] [FILE: runtime.cc][LINE:2060]\n  Start aicpu executor failed, retCode=Ox7020009 devId=0[FUNC:DeviceRetain][FILE: runtime.cc] [LINE:2698]\n  Check param failed, dev can not be NULL! [FUNC:PrimaryContextRetain] [FILE: runtime.cc][LINE:2544]\n  Check param failed, ctx can not be NULL! [FUNC:PrimaryContextRetain J[FILE: runtime.cc][LINE:2571]\n  Check param failed, context can not be null. [FUNC:NewDevice] [FILE:api_impl.cc][LINE: 1893]\n  New device failed, retCode=0x7010006[FUNC:SetDevice] [FILE:api impl.cc][LINE:1916]\n  rtSetDevice execute failed, reason=[device retain errorl[FUNC:FuncErrorReason] [FILE:error_message_manage.cc][LINE:50]\n  open device 0 failed, runtime result = 507033. [FUNC:ReportCalLError][FILE:log_inner.cpp]ILINE:161]\n(Please search \"Ascend Error Message\" at https://www.mindspore.cn for error code description)\n</code></pre> <p>Potential Reason:</p> <ul> <li> <p>The mismatch of Ascend driver and CANN</p> </li> <li> <p>Unsuccessfully configuration of environment variables make aicpu fail, try to add the following items to environment variables(<code>LD_LIBRARY_PATH</code>):</p> </li> </ul> <pre><code>export ASCEND_OPP_PATH=${ASCEND_HOME}/latest/opp\nexport ASCEND_AICPU_PATH=${ASCEND_OPP_PATH}/..\n</code></pre>"},{"location":"tutorials/frequently_asked_questions/#q4-acl-open-device-0-failed","title":"Q4 <code>acl open device 0 failed</code>","text":"<p>Problem <code>acl open device 0 failed</code> may occur when doing inference. For example</p> <pre><code>benchmark --modelFile=dbnet_mobilenetv3_lite.mindir --device=Ascend --inputShapes='1,3,736,1280' --loopCount=100 - -wammUpLoopCount=10\nModelPath = dbnet_mobilenetv3_lite.mindir\nModelType = MindIR\nInDatapath =\nGroupInfoFile =\nConfigFilepath =\nInDataType = bin\nLoopCount = 100\nDeviceType = Ascend\nAccuracyThreshold = 0.5\nCosineDistanceThreshold = -1.1\nWarmUpLoopCount = 10\nNumThreads = 2  InterOpParallelNum = 1\nFpl16Priority = 0   Enableparal\u00cdel = 0\ncalibDataPath =\nEnableGLTexture = 0\ncpuBindMode = HIGHER CPU\nCalibDataType = FLOAT\nResize Dims: 1 3 736 1280\nstart unified benchmark run\nIERROR] ME (26748,7f6c73867fc0, benchmark) :2023-10-26-09:51 : 54.833.515 Imindspore/lite/src/extend rt/kernel/ascend/model/model_infer.cc:59] Init] Acl open device 0 failed.\n[ERROR] ME (26748,7f6c73867fc0,benchmark):2023-10-26-09:51:54.833.573 [mindspore/lite/src/extend rt/kernel/ascend/src/custom_ascend_kernel.cc:141] Init] Model i\nnfer init failed.   [ERROR] ME (26748, 7f6c73867fc0,benchmark) :2023-10-26-09:51:54.833.604 [mindspore/lite/src/extendrt/session/single_op_session.cc:198] BuildCustomAscendKernelImpl] kernel init failed CustomAscend\n[ERROR] ME (26748,7f6c73867fc0, benchmark) :2023-10-26-09:51 :54.833.669 [mindspore/li te/src/extendrt/session/single_op_sess ion.cc:220] BuildCustomAscendKernel] Build ascend kernel failed for node: custom_0\n[ERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51 : 54.833.699 [mindspore/lite/src/extend rt/session/single_op_session.cc:302] CompileGraph] Failed to Build custom ascend kernel\n[ERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51:54.833.727 [mindspore/lite/s rc/extendrt/cxx_api/model/model_impl.cc:413] BuildByBufferImpl] compile graph failed.\n[ERROR] ME (26748, 7f6c73867fc0, benchmark):2023-10-26-09:51:54.835.590 [mindspore/lite/tools/benchma rk/benchmark_unified_api.cc:1256] CompileGraph] ms_model_.Build failed while running\nIERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51:54.835.627 [mindspore/lite/tools/benchma rk/benchmark_unified_api.cc:1325] RunBenchmark] Compile graph failed.\n[ERROR] ME(26748,7f6c73867fc0, benchmark):2023-10-26-09: 51:54.835.662 [mindspore/lite/tools/benchmark/ run_benchmark.cc :78] RunBenchmark] Run Benchmark dbnet_mobilenetv3_lite.mindi r Failed : -1\nms_model_.Build failed while running Run Benchmark dbnet mobilenetv3 lite.mindir Failed : -1\n</code></pre> <p>This problem occur when <code>acllib</code> library is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>). try to add the following items to environment variables.</p> <pre><code>export NPU_HOST_LIB=$ASCEND_HOME/latest/acllib/lib64/stub\nexport DDK_PATH=$ASCEND_HOME/latest\nexport LD_LIBRARY_PATH=$ASCEND_HOME/latest/acllib/lib64\nexport ASCEND_AICPU_PATH=$ASCEND_HOME/latest/x86_64-linux\nexport LD_LIBRARY_PATH=$ASCEND_HOME/latest/x86_64-linux/lib64:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"tutorials/frequently_asked_questions/#q5-fail-to-install-mindocr-dependency-on-windows","title":"Q5 Fail to install mindocr dependency on windows","text":"<p>Running the following command on windows</p> <pre><code>git clone git@gitee.com:mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>The error may occur with the following message, <code>lanms</code> package fail to install.</p> <pre><code>FileNotFoundError: [WinError 2]  the system cannot find the file specified.\n</code></pre> <p><code>lanma</code> from https://github.com/argman/EAST/  on windows may not work, and linux platform is recommanded. You can try to use <code>lanms-neo</code> to replace <code>lanms</code> on windows. You may find the following error when installing <code>lanms-neo</code>:</p> <pre><code>Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting lanms-neo==1.0.2\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7b/fe/beff7e7e4455cb9f69c5734897ca8552a57f6423b062ec86b2ebc1d79c0d/lanms_neo-1.0.2.tar.gz (39 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: lanms-neo\n  Building wheel for lanms-neo (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Building wheel for lanms-neo (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [10 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build\\lib.win-amd64-cpython-37\n      creating build\\lib.win-amd64-cpython-37\\lanms\n      copying lanms\\__init__.py -&gt; build\\lib.win-amd64-cpython-37\\lanms\n      running build_ext\n      building 'lanms._C' extension\n      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for lanms-neo\nFailed to build lanms-neo\nERROR: Could not build wheels for lanms-neo, which is required to install pyproject.toml-based projects\n</code></pre> <p>The Visual-cpp-build-tools should be installed first, and you can successfully run <code>pip install lanms-neo</code>.</p> <p>Remove the <code>lanms</code> item from <code>requirements.txt</code>, and run <code>pip install -r requirements.txt</code> to finish installation.</p>"},{"location":"tutorials/frequently_asked_questions/#q6-runtimeerror-the-device-address-type-is-wrong-type-name-in-addresscpu-type-name-in-contextascend","title":"Q6 <code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code>","text":"<ul> <li><code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code> may occur when exporting models:</li> </ul> <pre><code>[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.141.25 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.143.96 [mindspore/run_check/_check_version.py:460] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.144.71 [mindspore/run_check/_check_version.py:466] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\nTraceback (most recent call last):\n  File \"tools/export.py\", line 173, in &lt;module&gt;\n    export(**vars(args))\n  File \"tools/export.py\", line 73, in export\n    net = build_model(model_cfg, pretrained=True, amp_level=amp_level)\n  File \"/xxx/mindocr/mindocr/models/builder.py\", line 52, in build_model\n    network = create_fn(**kwargs)\n  File \"/xxx/mindocr/mindocr/models/rec_svtr.py\", line 122, in svtr_tiny_ch\n    model = SVTR(model_config)\n  File \"/xxx/mindocr/mindocr/models/rec_svtr.py\", line 26, in __init__\n    BaseModel.__init__(self, config)\n  File \"/xxx/mindocr/mindocr/models/base_model.py\", line 34, in __init__\n    self.backbone = build_backbone(backbone_name, **config.backbone)\n  File \"/xxx/mindocr/mindocr/models/backbones/builder.py\", line 48, in build_backbone\n    backbone = backbone_class(**kwargs)\n  File \"/xxx/mindocr/mindocr/models/backbones/rec_svtr.py\", line 486, in __init__\n    ops.zeros((1, num_patches, embed_dim[0]), ms.float32)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/function/array_func.py\", line 1039, in zeros\n    output = zero_op(size, value)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\", line 314, in __call__\n    return _run_op(self, self.name, args)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\", line 913, in _run_op\n    stub = _pynative_executor.run_op_async(obj, op_name, args)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/common/api.py\", line 1186, in run_op_async\n    return self._executor.run_op_async(*args)\nRuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_device_res_manager.cc:72 AllocateMemory\n</code></pre> <ul> <li>An error occurred in the calculation in Ascend mode, which raise <code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import mindspore as ms\n[WARNING] ME(44720:140507814819648,MainProcess):2023-11-01-03:01:38.884.384 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n[WARNING] ME(44720:140507814819648,MainProcess):2023-11-01-03:01:38.884.675 [mindspore/run_check/_check_version.py:466] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n&gt;&gt;&gt; import mindspore.ops as ops\n&gt;&gt;&gt; ms.set_context(device_target=\"Ascend\")\n&gt;&gt;&gt; ms.run_check()\nMindSpore version:  2.2.0.20231025\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n&gt;&gt;&gt; x = ms.Tensor(np.ones([1,3,3,4]).astype(np.float32))\n&gt;&gt;&gt; y = ms.Tensor(np.ones([1,3,3,4]).astype(np.float32))\n&gt;&gt;&gt; print(ops.add(x, y))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/_stub_tensor.py\", line 49, in fun\n    return method(*arg, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/tensor.py\", line 493, in __str__\n    return str(self.asnumpy())\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/tensor.py\", line 964, in asnumpy\n    return Tensor_.asnumpy(self)\nRuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_device_res_manager.cc:72 AllocateMemory\n</code></pre> <p>Ascend 310 or Ascend 310P3 may not support calculation in MindSpore Ascend mode. Please refer  MindSpore download and check the consistence of MindSpore version and platform. You can also</p> <ul> <li>Change Ascend mode to CPU</li> <li>Use MindSpore Lite instead</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q7-problems-related-to-model-converting","title":"Q7 Problems related to model converting","text":"<ul> <li><code>SetGraphInputShape] Failed to find input xxx in input_shape yyy:xxxxxxxxxxx</code> occurs when converting model to Device-side <code>mindir</code> by <code>converter_lite</code>. If you convert dbnet_resnet50.mindir to Device-side <code>mindir</code> with the <code>config.txt</code> as following:</li> </ul> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=args0:[1,3,736,1280]\n</code></pre> <p>and run the following command to convert model:</p> <pre><code>converter_lite --saveType=MINDIR --fmk=MINDIR --optimize=ascend_oriented --modelFile=dbnet_resnet50.mindir --outputFile=dbnet_resnet50_lite --configFile=config.txt\n</code></pre> <p>Error may occur:</p> <pre><code>[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.385 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:756] SetGraphInputShape] Failed to find input x in input_shape args0:1,3,736,1280\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.416 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:773] ConvertGraphToOm] Failed to set graph input shape\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.427 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.439 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.450 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.461 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.476 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.486 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.555 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.564 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.006.118 [mindspore/lite/tools/converter/converter.cc:1029] HandleGraphCommon] Optimize func graph failed: -2 NULL pointer returned.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.133 [mindspore/lite/tools/converter/converter.cc:979] Convert] Handle graph failed: -2 NULL pointer returned.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.150 [mindspore/lite/tools/converter/converter.cc:1166] RunConverter] Convert model failed\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.163 [mindspore/lite/tools/converter/cxx_api/converter.cc:348] Convert] Convert model failed, ret=NULL pointer returned.\nERROR [mindspore/lite/tools/converter/converter_lite/main.cc:104] main] Convert failed. Ret: NULL pointer returned.\nConvert failed. Ret: NULL pointer returned.\n</code></pre> <p>The problem is caused by the mismatch of variable names. The error message</p> <pre><code>Failed to find input x in input_shape args0:1,3,736,1280\n</code></pre> <p>shows that variable <code>x</code> does not match with variable <code>args0</code>. Try to replace <code>args0</code> with <code>x</code> in the <code>config.txt</code>.</p> <ul> <li><code>Can't find OpAdapter for LSTM</code> occurs when converting model to <code>MindSpore Lite Mindir</code> by <code>converter_lite</code>.</li> </ul> <p>After exporting a MindIR file by <code>export.py</code> on lite inference devices, convert the MindIR file to <code>MindSpore Lite Mindir</code> by <code>converter_lite</code> as the following command:</p> <pre><code>converter_lite \\\n  --saveType=MINDIR \\\n  --fmk=MINDIR \\\n  --optimize=ascend_oriented \\\n  --modelFile=./models/rec/CRNN/VGG7/crnn_vgg7.mindir \\\n  --outputFile=./models/rec/CRNN/VGG7/crnn_vgg7_lite \\\n  --configFile=./config.txt\n</code></pre> <p>Error may occur:</p> <pre><code>[WARNING] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.361 [mindspore/ccsrc/transform/graph_ir/utils.cc:59] FindAdapter] Can't find OpAdapter for LSTM\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.393 [mindspore/ccsrc/transform/graph_ir/convert.cc:4040] ConvertCNode] Cannot get adapter for Default/neck-RNNEncoder/seq_encoder-LSTM/rnn-_DynamicLSTMCPUGPU/LSTM-op90\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.437 [mindspore/ccsrc/transform/graph_ir/convert.cc:1034] ConvertAllNode] Failed to convert node: @391_390_1_mindocr_models_base_model_BaseModel_construct_24_1:nout{[0]: ValueNode&lt;Primitive&gt; LSTM, [1]: nout, [2]: nout, [3]: nout, [4]: nout}.\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.457 [mindspore/ccsrc/transform/graph_ir/convert.cc:1034] ConvertAllNode] Failed to convert node: ValueNode&lt;Primitive&gt; TupleGetItem.\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.561 [mindspore/ccsrc/transform/graph_ir/convert.cc:1034] ConvertAllNode] Failed to convert node: @391_390_1_mindocr_models_base_model_BaseModel_construct_24_1:nout{[0]: ValueNode&lt;Primitive&gt; TupleGetItem, [1]: nout, [2]: ValueNode&lt;Int64Imm&gt; 0}.\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.582 [mindspore/ccsrc/transform/graph_ir/convert.cc:1034] ConvertAllNode] Failed to convert node: ValueNode&lt;Primitive&gt; ReverseV2.\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.667 [mindspore/ccsrc/transform/graph_ir/convert.cc:1034] ConvertAllNode] Failed to convert node: @391_390_1_mindocr_models_base_model_BaseModel_construct_24_1:nout{[0]: ValueNode&lt;Primitive&gt; ReverseV2, [1]: nout}.\n[ERROR] GE_ADPT(837950,7feb6e13bf40,converter_lite):2024-10-26-07:37:40.545.734 [mindspore/ccsrc/transform/graph_ir/convert.cc:1034] ConvertAllNode] Failed to convert node: @391_390_1_mindocr_models_base_model_BaseModel_construct_24_1:param_neck.seq_encoder.bias_hh_l0.\n</code></pre> <p>In this case, please try to export the MindIR file by <code>export.py</code> on Ascend training devices, and then convert the MindIR file to <code>MindSpore Lite Mindir</code> by <code>converter_lite</code> on lite inference devices.</p> <ul> <li><code>RuntimeError: Load op info form json config failed, version: Ascend310</code> occurs when export a MindIR file by <code>export.py</code>.</li> </ul> <p>Export a MindIR file by <code>export.py</code> on lite inference devices as the following command:</p> <pre><code>python tools/export.py \\\n      --model_name_or_config configs/det/fcenet/fce_icdar15.yaml \\\n      --data_shape 736 1280 \\\n      --local_ckpt_path ./fcenet_resnet50-43857f7f.ckpt\n</code></pre> <p>Error may occur:</p> <pre><code>  [ERROR] KERNEL(849474,7f7571d68740,python):2024-10-26-08:44:27.998.221 [mindspore/ccsrc/kernel/oplib/op_info_utils.cc:179] LoadOpInfoJson] Get op info json suffix path failed, soc_version: Ascend310\n  [ERROR] KERNEL(849474,7f7571d68740,python):2024-10-26-08:44:27.998.362 [mindspore/ccsrc/kernel/oplib/op_info_utils.cc:118] GenerateOpInfos] Load op info json failed, version: Ascend310\n  [ERROR] ANALYZER(849474,7f7571d68740,python):2024-10-26-08:44:30.168.028 [mindspore/ccsrc/pipeline/jit/ps/static_analysis/async_eval_result.cc:70] HandleException] Exception happened, check the information as below.\n  RuntimeError: Load op info form json config failed, version: Ascend310\n\n  ----------------------------------------------------\n  - C++ Call Stack: (For framework developers)\n  ----------------------------------------------------\n  mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_kernel_runtime.cc:320 Init\n</code></pre> <p>In this case, please try to export the MindIR file by <code>export.py</code> on Ascend training devices.</p> <ul> <li><code>Save ge model to buffer failed.</code> when using Cloud-Side <code>mindir</code> model to do inference.   For example, if you use Cloud-Side <code>mindir</code> model to do inference in detection stage, it may raise:</li> </ul> <pre><code>[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:45.206.120 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:200] operator()] Save ge model to buffer failed.\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:45.206.157 [mindspore/ccsrc/cxx_api/model/model_converter_utils/multi_process.cc:118] ParentProcess] Parent process process failed\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:45.277.253 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:200] operator()] Save ge model to buffer failed.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:45.277.292 [mindspore/ccsrc/cxx_api/model/model_converter_utils/multi_process.cc:118] ParentProcess] Parent process process failed\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.224 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:251] LoadMindIR] Convert MindIR model to OM model failed\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.280 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:781] ConvertGraphToOm] Model converter load mindir failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.307 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.332 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.359 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.388 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.430 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.453 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.673 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.698 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.216 [mindspore/lite/src/extendrt/convert/runtime_convert.cc:214] RuntimeConvert] Convert model failed\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.270 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:507] ConvertGraphOnline] Failed to converter graph\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.351 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:395] BuildByBufferImpl] convert graph failed.\n[ERROR] MINDOCR(43138:139649752078144,Process-1:18):2023-11-10-03:40:46.255.926 [src/parallel/framework/module_base.py:38] DetInferNode init failed: build_from_file failed! Error is Common error code.\nProcess Process-1:18:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 39, in process_handler\n    raise error\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 34, in process_handler\n    params = self.init_self_args()\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/detection/det_infer_node.py\", line 12, in init_self_args\n    self.text_detector.init(preprocess=False, model=True, postprocess=False)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_base.py\", line 29, in init\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_det.py\", line 22, in _init_model\n    device_id=self.args.device_id,\n  File \"/home/mindocr/deploy/py_infer/src/core/model/model.py\", line 15, in __init__\n    self.model = _INFER_BACKEND_MAP[backend](**kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 16, in __init__\n    super().__init__(model_path, device, device_id)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/model_base.py\", line 28, in __init__\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 33, in _init_model\n    self.model.build_from_file(self.model_path, mslite.ModelType.MINDIR, context)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 95, in warpper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n    raise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.698 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:251] LoadMindIR] Convert MindIR model to OM model failed\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.755 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:781] ConvertGraphToOm] Model converter load mindir failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.782 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.807 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.834 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.864 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.904 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.928 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.306.162 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.306.188 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.599 [mindspore/lite/src/extendrt/convert/runtime_convert.cc:214] RuntimeConvert] Convert model failed\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.646 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:507] ConvertGraphOnline] Failed to converter graph\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.712 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:395] BuildByBufferImpl] convert graph failed.\n[ERROR] MINDOCR(43123:139649752078144,Process-1:17):2023-11-10-03:40:46.324.506 [src/parallel/framework/module_base.py:38] DetPreNode init failed: build_from_file failed! Error is Common error code.\nProcess Process-1:17:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 39, in process_handler\n    raise error\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 34, in process_handler\n    params = self.init_self_args()\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/detection/det_pre_node.py\", line 13, in init_self_args\n    self.text_detector.init(preprocess=True, model=False, postprocess=False)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_base.py\", line 29, in init\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_det.py\", line 22, in _init_model\n    device_id=self.args.device_id,\n  File \"/home/mindocr/deploy/py_infer/src/core/model/model.py\", line 15, in __init__\n    self.model = _INFER_BACKEND_MAP[backend](**kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 16, in __init__\n    super().__init__(model_path, device, device_id)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/model_base.py\", line 28, in __init__\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 33, in _init_model\n    self.model.build_from_file(self.model_path, mslite.ModelType.MINDIR, context)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 95, in warpper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n    raise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n</code></pre> <p>Reason: - Cloud-Side <code>mindir</code> model should convert to Device-Side <code>mindir</code> model first. - Mismatch version of <code>converter_lite</code> tool and <code>mindspore_lite</code>. For example, it may fail when using <code>converter_lite 2.2</code> to get Device-Side <code>mindir</code>, and do inference with <code>mindspore_lite 2.1</code>.</p>"},{"location":"tutorials/frequently_asked_questions/#q8-problems-related-to-inference","title":"Q8 Problems related to inference","text":"<ul> <li>When doing inference with <code>deploy/py_infer/infer.py</code>, it may occur <code>TypeError: unhashable type: 'numpy.ndarray'</code></li> </ul> <pre><code>[ERROR] MINDOCR(51913:140354674829120,Process-1:28):2023-11-10-06:52:34.304.673 [src/parallel/framework/module_base.py:66] ERROR occurred in RecPostNode module for test.jpg: unhashable type: 'numpy.ndarray'.\nTraceback (most recent call last):\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 62, in call_process\n    self.process(send_data)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/recognition/rec_post_node.py\", line 24, in process\n    output = self.text_recognizer.postprocess(data[\"pred\"], batch)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_rec.py\", line 132, in postprocess\n    return self.postprocess_ops(pred)\n  File \"/home/mindocr/deploy/py_infer/src/data_process/postprocess/builder.py\", line 32, in __call__\n    return self._ops_func(*args, **kwargs)\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in __call__\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in &lt;listcomp&gt;\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in &lt;listcomp&gt;\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\nTypeError: unhashable type: 'numpy.ndarray'\n</code></pre> <p>This problem occurs due to shape error, please check:</p> <ul> <li>Use suitable model. For example, it may fail and pass detection model to <code>--rec_model_path</code> parameter.</li> <li>Use inference model(not training model) to do converting.</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q9-training-speed-of-dbnet-not-as-fast-as-expexted","title":"Q9 Training speed of DBNet not as fast as expexted","text":"<p>When traning DBNet series networks (including DBNet MobileNetV3, DBNet ResNet-18, DBNet ResNet-50, and DBNet++ ResNet-50) using following command, the training speed is not as fast as expexted. For instance, the training speed of DBNet MobileNetV3 can reach only 80fps which is slower than the expecting 100fps.</p> <pre><code>python tools/train.py -c configs/det/dbnet/db_mobilenetv3_icdar15.yaml\n</code></pre> <p>This problem is due to the complex data pre-processing procedures of DBNet. The data pre-processing procedures will become the performance bottleneck if the computation ability of a CPU core of the training server is relatively weak.</p> <p>Solutions</p> <ol> <li> <p>Try to set the <code>train.dataset.use_minddata</code> and <code>eval.dataset.use_minddata</code> in the configuration file to <code>True</code>. MindOCR will execute parts of data pre-processing procedures using MindSporeMindData:</p> <pre><code>...\ntrain:\n  ckpt_save_dir: './tmp_det'\n  dataset_sink_mode: True\n  dataset:\n    type: DetDataset\n    dataset_root: /data/ocr_datasets\n    data_dir: ic15/det/train/ch4_training_images\n    label_file: ic15/det/train/det_gt.txt\n    sample_ratio: 1.0\n    use_minddata: True                          &lt;-- Set this configuration\n...\neval:\n  ckpt_load_path: tmp_det/best.ckpt\n  dataset_sink_mode: False\n  dataset:\n    type: DetDataset\n    dataset_root: /data/ocr_datasets\n    data_dir: ic15/det/test/ch4_test_images\n    label_file: ic15/det/test/det_gt.txt\n    sample_ratio: 1.0\n    use_minddata: True                          &lt;-- Set this configuration\n...\n</code></pre> </li> <li> <p>Try to set the <code>train.loader.num_workers</code> in the configuration file to a larger value to enhance the number of threads fetching dataset if the training server has enough CPU cores:</p> <pre><code>...\ntrain:\n  ...\n  loader:\n    shuffle: True\n    batch_size: 10\n    drop_remainder: True\n    num_workers: 12                             &lt;-- Set this configuration\n...\n</code></pre> </li> </ol>"},{"location":"tutorials/frequently_asked_questions/#q10-error-about-libgomp-d22c30c5so100","title":"Q10 Error about <code>libgomp-d22c30c5.so.1.0.0</code>","text":"<p>The following error may occur when running mindocr <pre><code>ImportError: /root/mindocr_env/lib/python3.8/site-packages/sklearn/__check_build/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block\n</code></pre> You can try the following steps to fix it:  - search <code>libgomp-d22c30c5.so.1.0.0</code> in your python install path    <pre><code>cd /root/mindocr_env/lib/python3.8\nfind ~ -name libgomp-d22c30c5.so.1.0.0\n</code></pre>    and get the following search result:    <pre><code>/root/mindocr_env/lib/python3.8/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0\n</code></pre>  - Add the so file to environment variable <code>LD_PRELOAD</code> <pre><code>export LD_PRELOAD=/root/mindocr_env/lib/python3.8/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"tutorials/frequently_asked_questions/#q11-dataset-pipeline-error-when-training-abinet-on-lmdb-dataset","title":"Q11 Dataset Pipeline Error when training abinet on lmdb dataset","text":"<p>The following error may occur when training abinet on lmdb dataset <pre><code>mindocr.data.rec_lmdb_dataset WARNING - Error occurred during preprocess.\n Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'.\n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message:\n------------------------------------------------------------------\n[ERROR] No cast for the specified DataType was found.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers)\n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/py_func_op.cc(143).\n</code></pre> You can try the following steps to fix it:  - find the folder of mindspore package  - open file: <code>mindspore/dataset/transforms/transform.py</code>  - switch to line 93:   <pre><code>93        if key in EXECUTORS_LIST:\n94           # get the executor by process id and thread id\n95            executor = EXECUTORS_LIST[key]\n96            # remove the old transform which in executor and update the new transform\n97            executor.UpdateOperation(self.parse())\n98        else:\n99            # create a new executor by process id and thread_id\n100           executor = cde.Execute(self.parse())\n101           # add the executor the global EXECUTORS_LIST\n102           EXECUTORS_LIST[key] = executor\n</code></pre>  - replace line 97 with <code>executor = cde.Execute(self.parse())</code>, and get   <pre><code>93        if key in EXECUTORS_LIST:\n94            # get the executor by process id and thread id\n95            executor = EXECUTORS_LIST[key]\n96            # remove the old transform which in executor and update the new transform\n97            executor = cde.Execute(self.parse())\n98        else:\n99            # create a new executor by process id and thread_id\n100           executor = cde.Execute(self.parse())\n101           # add the executor the global EXECUTORS_LIST\n102           EXECUTORS_LIST[key] = executor\n</code></pre>   - save the file, and try to train the model.</p>"},{"location":"tutorials/frequently_asked_questions/#q12-runtime-error-when-training-dbnet-on-synthtext-dataset","title":"Q12 Runtime Error when training dbnet on synthtext dataset","text":"<p>Runtime Error occur as following when training dbnet on synthtext dataset: <pre><code>Traceback (most recent call last):\n  ...\n  File \"/root/archiconda3/envs/Python380/lib/python3.8/site-packages/mindspore/common/api.py\", line 1608, in _exec_pip\n    return self.graph_executor(args, phase)\nRuntimeError: Run task for graph:kernel_graph_1 error! The details reger to 'Ascend Error Message'\n</code></pre> Please update CANN to 7.1 version.</p>"},{"location":"tutorials/frequently_asked_questions/#q13-failed-to-install-seqeval","title":"Q13 Failed to install seqeval","text":"<p>The following error occur when run <code>pip install -r requirements.txt</code> <pre><code>Collecting seqeval&gt;=1.2.2 (from -r requirements.txt (line 19))\n  Downloading http://mirrors.aliyun.com/pypi/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 43.6/43.6 kB 181.0 kB/s eta 0:00:00\n  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [48 lines of output]\n      /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n      !!\n\n              ********************************************************************************\n              Requirements should be satisfied by a PEP 517 installer.\n              If you are using pip, you can try `pip install --use-pep517`.\n              ********************************************************************************\n\n      !!\n        dist.fetch_build_eggs(dist.setup_requires)\n      WARNING: The repository located at mirrors.aliyun.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host mirrors.aliyun.com'.\n      ERROR: Could not find a version that satisfies the requirement setuptools_scm (from versions: none)\n      ERROR: No matching distribution found for setuptools_scm\n      Traceback (most recent call last):\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/installer.py\", line 101, in _fetch_build_egg_no_warn\n          subprocess.check_call(cmd)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/subprocess.py\", line 373, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmpusgt0k69', '--quiet', 'setuptools_scm']' returned non-zero exit status 1.\n\n      The above exception was the direct cause of the following exception:\n\n      Traceback (most recent call last):\n        File \"&lt;string&gt;\", line 2, in &lt;module&gt;\n        File \"&lt;pip-setuptools-caller&gt;\", line 34, in &lt;module&gt;\n        File \"/tmp/pip-install-m2kqztlz/seqeval_da00f708dc0e483b92cd18083513d5e7/setup.py\", line 27, in &lt;module&gt;\n          setup(\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/__init__.py\", line 102, in setup\n          _install_setup_requires(attrs)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/__init__.py\", line 75, in _install_setup_requires\n          _fetch_build_eggs(dist)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/__init__.py\", line 80, in _fetch_build_eggs\n          dist.fetch_build_eggs(dist.setup_requires)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/dist.py\", line 636, in fetch_build_eggs\n          return _fetch_build_eggs(self, requires)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/installer.py\", line 38, in _fetch_build_eggs\n          resolved_dists = pkg_resources.working_set.resolve(\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 829, in resolve\n          dist = self._resolve_dist(\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 865, in _resolve_dist\n          dist = best[req.key] = env.best_match(\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 1135, in best_match\n          return self.obtain(req, installer)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 1147, in obtain\n          return installer(requirement)\n        File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/installer.py\", line 103, in _fetch_build_egg_no_warn\n          raise DistutilsError(str(e)) from e\n      distutils.errors.DistutilsError: Command '['/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmpusgt0k69', '--quiet', 'setuptools_scm']' returned non-zero exit status 1.\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500&gt; See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\n</code></pre> Please try the following steps to fix this problem\uff1a  - Update <code>setuptools</code>: <code>pip3 install --upgrade setuptools</code>  - Update <code>setuptools_scm</code>: <code>pip3 install --upgrade setuptools_scm</code>  - Install <code>seqeval</code>\uff1a<code>pip3 install seqeval -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p>"},{"location":"tutorials/frequently_asked_questions/#q14-failed-to-install-lanms","title":"Q14 Failed to install lanms","text":"<p>The following error occur when installing lanms <pre><code>ImportError: Python version mismatch: module was compiled for version 3.8, while the interpreter is running version 3.7.\n</code></pre> Some Python 3.7 environment may meet this problem when multiple python3 environment exists. You could try the following steps to solve this problem: 1. run <code>pip3 install lanms -i https://pypi.tuna.tsinghua.edu.cn/simple</code>, and get the url for downloading <code>lanms-1.0.2.tar.gz</code>(like https://pypi.tuna.tsinghua.edu.cn/packages/96/c0/50dc2c857ed060e907adaef31184413a7706e475c322236d346382e45195/lanms-1.0.2.tar.gz) 2. use this url and dowload the <code>lanms-1.0.2.tar.gz</code>, run <code>tar -zxvf lanms-1.0.2.tar.gz</code> to decompress the package. 3. <code>cd lanms-1.0.2</code> 4. edit the <code>Makefile</code>, replace <code>python3-config</code> with <code>python3.7-config</code> in line 1 and line 2, and you could get    <pre><code>CXXFLAGS = -I include  -std=c++11 -O3 $(shell python3.7-config --cflags)\nLDFLAGS = $(shell python3.7-config --ldflags)\n...\n</code></pre>    save <code>Makefile</code>. So that the make process would exactly compile with python3.7 environment 5. run <code>python setup.py install</code> and completely install lanms.</p> <p>dca11cc9989deabe86985f0729502266e5ba6f42</p>"},{"location":"tutorials/training_detection_custom_dataset/","title":"Training Detection Network with Custom Datasets","text":"<p>This document provides tutorials on how to train text detection networks using custom datasets.</p> <ul> <li>Training Detection Network with Custom Datasets</li> <li>1. Dataset preperation<ul> <li>1.1 Preparing Training Data</li> <li>1.2 Preparing Validation Data</li> </ul> </li> <li>2. Configuration File Preperation<ul> <li>2.1 Configure train/validation datasets</li> <li>2.2 Configure train/validation transform pipelines</li> <li>2.3 Configure the model architecture</li> <li>2.4 Configure training hyperparameters</li> </ul> </li> <li>3. Model Training, Evaluation, and Inference<ul> <li>3.1 Training</li> <li>3.2 Evaluation</li> <li>3.3 Inference</li> <li>3.3.1 Environment Preparation</li> <li>3.3.2 Model Conversion</li> <li>3.3.3 Inference (Python)</li> </ul> </li> </ul>"},{"location":"tutorials/training_detection_custom_dataset/#1-dataset-preperation","title":"1. Dataset preperation","text":"<p>Currently, MindOCR detection network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images, text bounding boxes, and transcriptions. An example of the target file format is: <pre><code>img_1.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>It is read by DetDataset. If your dataset is not in the same format as the example format, see instructions on how convert different datasets' annotations into the supported format.</p> <ul> <li><code>SynthTextDataset</code>: A file format provided by SynthText800k. More details about this dataset can be found here. The annotation file is a <code>.mat</code> file consisting of <code>imnames</code>(image names), <code>wordBB</code>(word-level bounding-boxes), <code>charBB</code>(character-level bounding boxes), and <code>txt</code> (text strings). It is read by SynthTextDataset. Users can take <code>SynthTextDataset</code> as a reference to write their custom dataset class.</li> </ul> <p>We recommend users to prepare text detection datasets in the <code>Common Dataset</code> format, and then use DetDataset to load the data. The following tutorials further explain on the detailed steps.</p>"},{"location":"tutorials/training_detection_custom_dataset/#11-preparing-training-data","title":"1.1 Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file <code>train_det.txt</code> at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # A list of dictionaries\nimg_1.jpg\\t[{\"transcription\": \"Genaxis Theatre\", \"points\": [[377, 117], [463, 117], [465, 130], [378, 130]]}, {\"transcription\": \"[06]\", \"points\": [[493, 115], [519, 115], [519, 131], [493, 131]]}, {...}]\nimg_2.jpg\\t[{\"transcription\": \"guardian\", \"points\": [[642, 250], [769, 230], [775, 255], [648, 275]]}]\n...\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- train_det.txt\n    |- training\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#12-preparing-validation-data","title":"1.2 Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file <code>val_det.txt</code> at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- val_det.txt\n    |- validation\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#2-configuration-file-preperation","title":"2. Configuration File Preperation","text":"<p>To prepare the corresponding configuration file, users should specify the directories for the training and validation datasets.</p>"},{"location":"tutorials/training_detection_custom_dataset/#21-configure-trainvalidation-datasets","title":"2.1 Configure train/validation datasets","text":"<p>Please select <code>configs/det/dbnet/db_r50_icdar15.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: DetDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: train_det.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\n  dataset:\n    type: DetDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: val_det.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n  ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#22-configure-trainvalidation-transform-pipelines","title":"2.2 Configure train/validation transform pipelines","text":"<p>Take the <code>train.dataset.transform_pipeline</code> field in the <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as an example. It specifies a set of transformations applied on the image or labels to generate the data as the model inputs or the loss function inputs. These transform functions are defined in <code>mindocr/data/transforms</code>.</p> <pre><code>...\ntrain:\n...\n  dataset:\n    transform_pipeline:\n      - DecodeImage:\n          img_mode: RGB\n          to_float32: False\n      - DetLabelEncode:\n      - RandomColorAdjust:\n          brightness: 0.1255  # 32.0 / 255\n          saturation: 0.5\n      - RandomHorizontalFlip:\n          p: 0.5\n      - RandomRotate:\n          degrees: [ -10, 10 ]\n          expand_canvas: False\n          p: 1.0\n      - RandomScale:\n          scale_range: [ 0.5, 3.0 ]\n          p: 1.0\n      - RandomCropWithBBox:\n          max_tries: 10\n          min_crop_ratio: 0.1\n          crop_size: [ 640, 640 ]\n          p: 1.0\n      - ValidatePolygons:\n      - ShrinkBinaryMap:\n          min_text_size: 8\n          shrink_ratio: 0.4\n      - BorderMap:\n          shrink_ratio: 0.4\n          thresh_min: 0.3\n          thresh_max: 0.7\n      - NormalizeImage:\n          bgr_to_rgb: False\n          is_hwc: True\n          mean: imagenet\n          std: imagenet\n      - ToCHWImage:\n  ...\n</code></pre> <ul> <li> <p><code>DecodeImage</code> and <code>DetLabelEncode</code>: the two transform functions parse the strings in <code>train_det.txt</code> file, load both the image and the labels, and save them as a dictionary;</p> </li> <li> <p><code>RandomColorAdjust</code>,  <code>RandomHorizontalFlip</code>, <code>RandomRotate</code>, <code>RandomScale</code>, and <code>RandomCropWithBBox</code>: these transform functions perform typical image augmentation operations. Except for <code>RandomColorAdjust</code>, all other functions alter the bounding box labels;</p> </li> <li> <p><code>ValidatePolygons</code>: it filters out the bounding boxes that are outside of the image due to previous augmentations;</p> </li> <li> <p><code>ShrinkBinaryMap</code> and <code>BorderMap</code>: they make the binary map and the border map needed for dbnet training;</p> </li> <li> <p><code>NormalizeImage</code>: it normalizes the image by the mean and variance of the ImageNet dataset;</p> </li> <li> <p><code>ToCHWImage</code>: it changes <code>HWC</code> images to <code>CHW</code> images.</p> </li> </ul> <p>For validation transform pipeline, all image augmentation operations are removed, and replaced by a simple resize function:</p> <p><pre><code>eval:\n  dataset\n    transform_pipeline:\n      - DecodeImage:\n          img_mode: RGB\n          to_float32: False\n      - DetLabelEncode:\n      - DetResize:\n          target_size: [ 736, 1280 ]\n          keep_ratio: False\n          force_divisable: True\n      - NormalizeImage:\n          bgr_to_rgb: False\n          is_hwc: True\n          mean: imagenet\n          std: imagenet\n      - ToCHWImage:\n</code></pre> More tutorials on transform functions can be found in the transform tutorial.</p>"},{"location":"tutorials/training_detection_custom_dataset/#23-configure-the-model-architecture","title":"2.3 Configure the model architecture","text":"<p>Although different models have different architectures, MindOCR formulates them as a general three-stage architecture: <code>[backbone]-&gt;[neck]-&gt;[head]</code>. Take <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as an example:</p> <p><pre><code>model:\n  type: det\n  transform: null\n  backbone:\n    name: det_resnet50  # Only ResNet50 is supported at the moment\n    pretrained: True    # Whether to use weights pretrained on ImageNet\n  neck:\n    name: DBFPN         # FPN part of the DBNet\n    out_channels: 256\n    bias: False\n    use_asf: False      # Adaptive Scale Fusion module from DBNet++ (use it for DBNet++ only)\n  head:\n    name: DBHead\n    k: 50               # amplifying factor for Differentiable Binarization\n    bias: False\n    adaptive: True      # True for training, False for inference\n</code></pre>  The backbone, neck, and head modules are all defined under <code>mindocr/models/backbones</code>, <code>mindocr/models/necks</code>, and <code>mindocr/models/heads</code>.</p>"},{"location":"tutorials/training_detection_custom_dataset/#24-configure-training-hyperparameters","title":"2.4 Configure training hyperparameters","text":"<p>Some training hyperparameters in <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> are defined as follows: <pre><code>metric:\n  name: DetMetric\n  main_indicator: f-score\n\nloss:\n  name: DBLoss\n  eps: 1.0e-6\n  l1_scale: 10\n  bce_scale: 5\n  bce_replace: bceloss\n\nscheduler:\n  scheduler: polynomial_decay\n  lr: 0.007\n  num_epochs: 1200\n  decay_rate: 0.9\n  warmup_epochs: 3\n\noptimizer:\n  opt: SGD\n  filter_bias_and_bn: false\n  momentum: 0.9\n  weight_decay: 1.0e-4\n</code></pre> It uses <code>SGD</code> optimizer (in <code>mindocr/optim/optim.factory.py</code>) and <code>polynomial_decay</code> (in <code>mindocr/scheduler/scheduler_factory.py</code>) as the learning scheduler. The loss function is <code>DBLoss</code> (in <code>mindocr/losses/det_loss.py</code>) and the evaluation metric is <code>DetMetric</code> ( in <code>mindocr/metrics/det_metrics.py</code>).</p>"},{"location":"tutorials/training_detection_custom_dataset/#3-model-training-evaluation-and-inference","title":"3. Model Training, Evaluation, and Inference","text":"<p>When all configurations have been specified, users can start training their models. MindOCR supports evaluation and inference after the model is trained.</p>"},{"location":"tutorials/training_detection_custom_dataset/#31-training","title":"3.1 Training","text":"<ul> <li>Standalone training</li> </ul> <p>In standalone training, the model is trained on a single device (<code>device:0</code> by default). Users should set <code>system.distribute</code> in yaml config file to be <code>False</code>, and the <code>system.device_id</code> to the target device id if users want to run this model on a device other than <code>device:0</code>.</p> <p>Take <code>configs/det/dbnet/db_r50_icdar15.yaml</code> as an example, the training command is:</p> <pre><code>python tools/train.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <ul> <li>Distributed training</li> </ul> <p>In distributed training, <code>distribute</code> in yaml config file should be True. On both GPU and Ascend devices, users can use <code>mpirun</code> to launch distributed training. For example, using <code>device:0</code> and <code>device:1</code> to train:</p> <pre><code># n is the number of GPUs/NPUs\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>Sometimes, users may want to specify the device ids to run distributed training, for example, <code>device:2</code> and <code>device:3</code>.</p> <p>On GPU devices, before running the <code>mpirun</code> command above, users can run the following command:</p> <pre><code>export CUDA_VISIBLE_DEVICES=2,3\n</code></pre> <p>On Ascend devices, users should create a <code>rank_table.json</code> like this: <pre><code>Copy{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"10.155.111.140\",\n            \"device\": [\n                {\"device_id\": \"2\",\"device_ip\": \"192.3.27.6\",\"rank_id\": \"2\"},\n                {\"device_id\": \"3\",\"device_ip\": \"192.4.27.6\",\"rank_id\": \"3\"}],\n             \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> To get the <code>device_ip</code> of the target device, run <code>cat /etc/hccn.conf</code> and look for the value of <code>address_x</code>, which is the ip address. More details can be found in distributed training tutorial.</p>"},{"location":"tutorials/training_detection_custom_dataset/#32-evaluation","title":"3.2 Evaluation","text":"<p>To evaluate the accuracy of the trained model, users can use <code>tools/eval.py</code>.</p> <p>Take standalone evaluation as an example. In the yaml config file, <code>system.distribute</code> should be <code>False</code>; the <code>eval.ckpt_load_path</code> should be the target ckpt path; <code>eval.dataset_root</code>, <code>eval.data_dir</code>, and <code>eval.label_file</code> should be correctly specified. Then the evaluation can be started by running:</p> <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>MindOCR also supports to specify the arguments in the command line, by running: <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml \\\n            --opt eval.ckpt_load_path=\"/path/to/local_ckpt.ckpt\" \\\n                  eval.dataset_root=\"/path/to/val_set/root\" \\\n                  eval.data_dir=\"val_set/dir\"\\\n                  eval.label_file=\"val_set/label\"\n</code></pre></p>"},{"location":"tutorials/training_detection_custom_dataset/#33-inference","title":"3.3 Inference","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend. Inference Tutorial gives detailed steps on how to run inference with MindOCR, which include mainly three steps: environment preparation, model conversion, and inference.</p>"},{"location":"tutorials/training_detection_custom_dataset/#331-environment-preparation","title":"3.3.1 Environment Preparation","text":"<p>Please refer to the environment installation for more information, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"tutorials/training_detection_custom_dataset/#332-model-conversion","title":"3.3.2 Model Conversion","text":"<p>Before runing infernence, users need to export a MindIR file from the trained checkpoint. MindSpore IR (MindIR) is a function-style IR based on graph representation. The MindIR filew stores the model structure and weight parameters needed for inference.</p> <p>Given the trained dbnet checkpoint file, user can use the following commands to export MindIR:</p> <pre><code>python tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n# or\npython tools/export.py --model_name_or_config configs/det/dbnet/db_r50_icdar15.yaml --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n</code></pre> <p>The <code>data_shape</code> is the model input shape of height and width for MindIR file. It may change when the model is changed.</p> <p>Please refer to the Conversion Tutorial for more details about model conversion.</p>"},{"location":"tutorials/training_detection_custom_dataset/#333-inference-python","title":"3.3.3 Inference (Python)","text":"<p>After model conversion, the <code>output.mindir</code> is obtained. Users can go to the <code>deploy/py_infer</code> directory, and use the following command for inference:</p> <pre><code>python infer.py \\\n    --input_images_dir=/your_path_to/test_images \\\n    --device=Ascend \\\n    --device_id=0 \\\n    --det_model_path=your_path_to/output.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --backend=lite \\\n    --res_save_dir=results_dir\n</code></pre> <p>Please refer to the Inference Tutorials chapter <code>4.1 Command example</code> on more examples of inference commands.</p>"},{"location":"tutorials/training_on_openi/","title":"Training on openi","text":""},{"location":"tutorials/training_on_openi/#mindocr-openi-training-guideline","title":"MindOCR OpenI Training Guideline","text":"<p>This tutorial introduces the training method of MindOCR using the OpenI platform.</p>"},{"location":"tutorials/training_on_openi/#clone-the-project","title":"Clone the project","text":"<p>Click on the plus sign and choose to New Migration to clone MindOCR from GitHub to the Openi platform.</p> <p>Enter the MindOCR git url: https://github.com/mindspore-lab/mindocr.git</p>"},{"location":"tutorials/training_on_openi/#prepare-dataset","title":"Prepare Dataset","text":"<p>You can upload your own dataset or associate the project with existing datasets on the platform.</p> <p>Uploading personal datasets requires setting the available clusters to NPU.</p>"},{"location":"tutorials/training_on_openi/#prepare-pretrained-model-optional","title":"Prepare pretrained model (optional)","text":"<p>To upload pre-trained weights, choose the Model tab of your repository.</p> <p>During the import of a local model, set the model's framework to MindSpore.</p>"},{"location":"tutorials/training_on_openi/#new-training-task","title":"New Training Task","text":"<p>Select Training Task -&gt; New Training Task in the Cloudbrain tab.</p> <p>In computing resources choose Ascend NPU.</p> <p>Set the training entry point (Start File) and add run parameters.</p> <ul> <li>To load pre-trained weights, choose the uploaded previously model file in the Select Model field and add <code>ckpt_dir</code> to the run parameters. The <code>ckpt_dir</code> parameter must have the following path: <code>/cache/*.ckpt</code>, where <code>*</code> is the model's file name.</li> <li>In the AI engine, it is necessary to select MindSpore version 1.9 or higher, and set the start file to <code>tools/train.py</code></li> <li>:warning: It is necessary to set <code>enable_modelarts</code> to <code>True</code> in the run parameters.</li> <li>The model's architecture is specified in the <code>config</code> file set in the run parameters. The prefix of the file is always <code>/home/work/user-job-dir/run-version-number</code>, where <code>run-version-number</code> for the newly created training task is usually <code>V0001</code>.</li> </ul>"},{"location":"tutorials/training_on_openi/#modify-existing-training-tasks","title":"Modify existing training tasks","text":"<p>Click the modify button of an existing training task to modify its parameters and run a new training task.</p> <p>Note: <code>run-version-number</code> will change to Parents Version (current run version number) + 1, e.g. <code>V0002</code>.</p>"},{"location":"tutorials/training_on_openi/#view-training-status","title":"View training status","text":"<p>Select a training task to view configuration information, logs, resource occupancy, and download model weights.</p>"},{"location":"tutorials/training_on_openi/#reference","title":"Reference","text":"<p>[1] Modified from https://github.com/mindspore-lab/mindyolo/blob/master/tutorials/cloud/openi.md</p>"},{"location":"tutorials/training_recognition_custom_dataset/","title":"Training Recognition Network with Custom Datasets","text":"<p>This document provides tutorials on how to train recognition networks using custom datasets, including the training of recognition networks in Chinese and English languages.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#dataset-preperation","title":"Dataset preperation","text":"<p>Currently, MindOCR recognition network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images and text files. It is read by RecDataset. - <code>LMDB Dataset</code>: A file format provided by LMDB. It is read by LMDBDataset.</p> <p>The following tutorials take the use of the <code>Common Dataset</code> file format as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-training-data","title":"Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # Corresponding label\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-validation-data","title":"Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#dictionary-preperation","title":"Dictionary Preperation","text":"<p>To train recognition networks for different languages, users need to configure corresponding dictionaries. Only characters that exist in the dictionary will be correctly predicted by the model. MindOCR currently provides three dictionaries, corresponding to Default, Chinese and English respectively. - <code>Default Dictionary</code>\uff1aincludes lowercase English letters and numbers only. If users do not configure the dictionay, this one will be used by default. - <code>English Dictionary</code>\uff1aincludes uppercase and lowercase English letters, numbers and punctuation marks, it is place at <code>mindocr/utils/dict/en_dict.txt</code>. - <code>Chinese Dictionary</code>\uff1aincludes commonly used Chinese characters, uppercase and lowercase English letters, numbers, and punctuation marks, it is placed at <code>mindocr/utils/dict/ch_dict.txt</code>.</p> <p>Currently, MindOCR does not provide a dictionary configuration for other languages. This feature will be released in a upcoming version.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configuration-file-preperation","title":"Configuration File Preperation","text":"<p>To configure the corresponding configuration file for a specific network architecture, users need to provide the necessary settings. As an example, we can take CRNN (with backbone Resnet34) as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-english-model","title":"Configure an English Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n  ...\n</code></pre> <p>And also modify the corresponding dictionary location to the the English dictionary path.</p> <pre><code>...\ncommon:\n  character_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>To use the complete English dictionary, users need to modify the <code>common:num_classes</code> attribute in the corresponding configuration file, as the initial configuration file\u2019s dictionary only includes lowercase English and numbers.</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 95                                        # The number is equal to the number of dictionary characters plus 1\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 96                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\n  use_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-english-dictionary","title":"Configuring a custom English dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-chinese-model","title":"Configure an Chinese Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34_ch.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n  ...\n</code></pre> <p>And also modify the corresponding dictionary location to the the Chinese dictionary path.</p> <pre><code>...\ncommon:\n  character_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 6625                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\n  use_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-chinese-dictionary","title":"Configuring a custom Chinese dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-training","title":"Model Training","text":"<p>When all datasets and configuration files are ready, users can start training models with their own data. As each model has different training methods, users can refer to the corresponding model introduction documentation for the Model Training and Model Evaluation sections. Here, we will only use CRNN as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-pre-trained-model","title":"Preparing Pre-trained Model","text":"<p>Users can use the pre-trained models that we provide as a starting point for training. Pre-trained models can often improve the convergence speed and even accuracy of the model. Taking the Chinese model as an example, the url for the pre-trained model that we provide is https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt. Users only need to add <code>model.pretrained</code> with the corresponding url in the configuration file as follows:</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  pretrained: https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>If users encounter network issues, they can try downloading the pre-trained model to their local machine in advance, and then change <code>model.pretrained</code> to the local path as follows:</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  pretrained: /local_path_to_the_ckpt/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>If users do not need to use the pre-trained model, they can simply delete <code>model.pretrained</code>.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#start-training","title":"Start Training","text":""},{"location":"tutorials/training_recognition_custom_dataset/#distributed-training","title":"Distributed Training","text":"<p>In the case of a large amount of data, we recommend that users use distributed training. For distributed training across multiple Ascend 910 devices or GPU devices, please modify the configuration parameter <code>system.distribute</code> to True, for example:</p> <pre><code># To perform distributed training on 4 GPU/Ascend devices\nmpirun -n 4 python tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#single-device-training","title":"Single Device Training","text":"<p>If you want to train or fine-tune the model on a smaller dataset without distributed training, please modify the configuration parameter <code>system.distribute</code> to <code>False</code> and run:</p> <pre><code># Training on single CPU/GPU/Ascend devices\npython tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>The training results (including checkpoint, performance of each epoch, and curve graph) will be saved in the directory configured by the <code>train.ckpt_save_dir</code> parameter in the YAML configuration file, which is set to <code>./tmp_rec</code> by default.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#resuming-training-from-checkpoint","title":"Resuming Training From Checkpoint","text":"<p>If users expect to load the optimizer, learning rate, and other information of the model while starting or continue training, they can add <code>model.resume</code> to the corresponding local model path in the configuration file as follows, and start training:</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  resume: /local_path_to_the_ckpt/model.ckpt\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Some models (including CRNN, RARE, SVTR) support mixed precision training to accelerate training speed. Users can try setting the <code>system.amp_level</code> in the configuration file to <code>O2</code> to start mixed precision training, as shown in the following example:</p> <pre><code>system:\n  mode: 0\n  distribute: True\n  amp_level: O2  # Mixed precision training\n  amp_level_infer: O2\n  seed: 42\n  log_interval: 100\n  val_while_train: True\n  drop_overflow_update: False\n  ckpt_max_keep: 5\n...\n</code></pre> <p>To disable mixed precision training, change <code>system.amp_level</code> to <code>O0</code>.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-evaluation","title":"Model Evaluation","text":"<p>To evaluate the accuracy of a trained model, users can use <code>tools/eval.py</code>. Please set the <code>ckpt_load_path</code> parameter in the <code>eval</code> section of the configuration file to the file path of the model checkpoint, and set <code>system.distribute</code> to False, as shown below:</p> <pre><code>system:\n  distribute: False # During evaluation stage, set to False\n...\neval:\n  ckpt_load_path: /local_path_to_the_ckpt/model.ckpt\n</code></pre> <p>and run</p> <pre><code>python tools/eval.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>You will get a model evaluation result similar to the following:</p> <pre><code>2023-06-16 03:41:20,237:INFO:Performance: {'acc': 0.821939, 'norm_edit_distance': 0.917264}\n</code></pre> <p>The number corresponding to <code>acc</code> is the accuracy of the model.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-inference","title":"Model Inference","text":"<p>Users can quickly obtain the inference results of the model by using the inference script. First, place the images in the same folder, and then execute:</p> <pre><code>python tools/infer/text/predict_rec.py --image_dir {dir_to_your_image_data} --rec_algorithm CRNN_CH --draw_img_save_dir inference_results\n</code></pre> <p>The results will be stored in <code>draw_img_save_dir/rec_results.txt</code>. Here are some examples:</p> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>You will get inference results similar to the following:</p> <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre>"},{"location":"tutorials/transform_tutorial/","title":"Transformation Tutorial","text":""},{"location":"tutorials/transform_tutorial/#mechanism","title":"Mechanism","text":"<ol> <li>Each transformation is a class with a callable function. An example is as follows</li> </ol> <pre><code>class ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    required keys: image\n    modified keys: image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data: dict):\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre> <ol> <li> <p>The input for transformation is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>The transformation api should have clarify the required keys in input and the modified or/and added keys in output the data dict.</p> </li> </ol> <p>Available transformations can be checked in <code>mindocr/data/transforms/*_transform.py</code></p> <pre><code># import and check available transforms\n\nfrom mindocr.data.transforms import general_transforms, det_transforms, rec_transforms\n</code></pre> <pre><code>general_transforms.__all__\n</code></pre> <pre><code>['DecodeImage', 'NormalizeImage', 'ToCHWImage', 'PackLoaderInputs']\n</code></pre> <pre><code>det_transforms.__all__\n</code></pre> <pre><code>['DetLabelEncode',\n 'MakeBorderMap',\n 'MakeShrinkMap',\n 'EastRandomCropData',\n 'PSERandomCrop']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#text-detection","title":"Text detection","text":""},{"location":"tutorials/transform_tutorial/#1-load-image-and-annotations","title":"1. Load image and annotations","text":""},{"location":"tutorials/transform_tutorial/#preparation","title":"Preparation","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%reload_ext autoreload\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\n\n# load the label file which has the info of image path and annotation.\n# This file is generated from the ic15 annotations using the converter script.\nlabel_fp = '/Users/Samit/Data/datasets/ic15/det/train/train_icdar2015_label.txt'\nroot_dir = '/Users/Samit/Data/datasets/ic15/det/train'\n\ndata_lines = []\nwith open(label_fp, 'r') as f:\n    for line in f:\n        data_lines.append(line)\n\n# just pick one image and its annotation\nidx = 3\nimg_path, annot = data_lines[idx].strip().split('\\t')\n\nimg_path = os.path.join(root_dir, img_path)\nprint('img_path', img_path)\nprint('raw annotation: ', annot)\n</code></pre> <pre><code>img_path /Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_612.jpg\nraw annotation:  [{\"transcription\": \"where\", \"points\": [[483, 197], [529, 174], [530, 197], [485, 221]]}, {\"transcription\": \"people\", \"points\": [[531, 168], [607, 136], [608, 166], [532, 198]]}, {\"transcription\": \"meet\", \"points\": [[613, 128], [691, 100], [691, 131], [613, 160]]}, {\"transcription\": \"###\", \"points\": [[695, 299], [888, 315], [931, 635], [737, 618]]}, {\"transcription\": \"###\", \"points\": [[709, 19], [876, 8], [880, 286], [713, 296]]}, {\"transcription\": \"###\", \"points\": [[530, 270], [660, 246], [661, 300], [532, 324]]}, {\"transcription\": \"###\", \"points\": [[113, 356], [181, 359], [180, 387], [112, 385]]}, {\"transcription\": \"###\", \"points\": [[281, 328], [369, 338], [366, 361], [279, 351]]}, {\"transcription\": \"###\", \"points\": [[66, 314], [183, 313], [183, 328], [68, 330]]}]\n</code></pre>"},{"location":"tutorials/transform_tutorial/#decode-the-image-decodeimage","title":"Decode the image  -  DecodeImage","text":"<pre><code>#img_path = '/Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_1.jpg'\ndecode_image = general_transforms.DecodeImage(img_mode='RGB')\n\n# TODO: check the input keys and output keys for the trans. func.\n\ndata = {'img_path': img_path}\ndata  = decode_image(data)\nimg = data['image']\n\n# visualize\nfrom mindocr.utils.visualize import show_img, show_imgs\nshow_img(img)\n</code></pre> <pre><code>import time\n\nstart = time.time()\natt = 100\nfor i in range(att):\n    img  = decode_image(data)['image']\navg = (time.time() - start) / att\n\nprint('avg reading time: ', avg)\n</code></pre> <pre><code>avg reading time:  0.004545390605926514\n</code></pre>"},{"location":"tutorials/transform_tutorial/#detlabelencode","title":"DetLabelEncode","text":"<pre><code>data['label'] = annot\n\ndecode_image = det_transforms.DetLabelEncode()\ndata = decode_image(data)\n\n#print(data['polys'])\nprint(data['texts'])\n\n# visualize\nfrom mindocr.utils.visualize import draw_boxes\n\nres = draw_boxes(data['image'], data['polys'])\nshow_img(res)\n</code></pre> <pre><code>['where', 'people', 'meet', '###', '###', '###', '###', '###', '###']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#2-image-and-annotation-processingaugmentation","title":"2. Image and annotation processing/augmentation","text":""},{"location":"tutorials/transform_tutorial/#randomcrop-eastrandomcropdata","title":"RandomCrop - EastRandomCropData","text":"<pre><code>from mindocr.data.transforms.general_transforms import RandomCropWithBBox\nimport copy\n\n#crop_data = det_transforms.EastRandomCropData(size=(640, 640))\ncrop_data = RandomCropWithBBox(crop_size=(640, 640))\n\nshow_img(data['image'])\nfor i in range(2):\n    data_cache = copy.deepcopy(data)\n    data_cropped = crop_data(data_cache)\n\n    res_crop = draw_boxes(data_cropped['image'], data_cropped['polys'])\n    show_img(res_crop)\n</code></pre>"},{"location":"tutorials/transform_tutorial/#colorjitter","title":"ColorJitter","text":"<pre><code>random_color_adj = general_transforms.RandomColorAdjust(brightness=0.4, saturation=0.5)\n\ndata_cache = copy.deepcopy(data)\n#data_cache['image'] = data_cache['image'][:,:, ::-1]\ndata_adj = random_color_adj(data_cache)\n#print(data_adj)\nshow_img(data_adj['image'], is_bgr_img=True)\n</code></pre>"},{"location":"tutorials/yaml_configuration/","title":"Configuration parameter description","text":"<ul> <li>system</li> <li>common</li> <li>model</li> <li>postprocess</li> <li>metric</li> <li>loss</li> <li>scheduler, optimizer, loss_scaler</li> <li>scheduler</li> <li>optimizer</li> <li>loss_scaler</li> <li>train, eval</li> <li>train</li> <li>eval</li> </ul> <p>This document takes <code>configs/rec/crnn/crnn_icdar15.yaml</code> as an example to describe the usage of parameters in detail.</p>"},{"location":"tutorials/yaml_configuration/#1-environment-parameters-system","title":"1. Environment parameters (system)","text":"Parameter Description Default Optional Values \u200b\u200b Remarks mode Mindspore running mode (static graph/dynamic graph) 0 0 / 1 0: means running in GRAPH_MODE mode; 1: PYNATIVE_MODE mode distribute Whether to enable parallel training True True / False \\ device_id Specify the device id while standalone training 7 The ids of all devices in the server Only valid when distribute=False (standalone training) and environment variable 'DEVICE_ID' is NOT set. While standalone training, if both this arg and environment variable 'DEVICE_ID' are NOT set, use device 0 by default. amp_level Mixed precision mode O0 O0/O1/O2/O3 'O0' - no change.  'O1' - convert the cells and operations in the whitelist to float16 precision, and keep the rest in float32 precision.  'O2' - Keep the cells and operations in the blacklist with float32 precision, and convert the rest to float16 precision.  'O3' - Convert all networks to float16 precision.  Notice: Model prediction or evaluation does not support 'O3' on GPU platform. If amp_level is set to 'O3' for model prediction and evaluation on GPU platform, the program will switch it to 'O2' automatically. seed Random seed 42 Integer \\ ckpt_save_policy The policy for saving model weights top_k \"top_k\" or \"latest_k\" \"top_k\" means to keep the top k checkpoints according to the metric score; \"latest_k\" means to keep the last k checkpoints. The value of <code>k</code> is set via <code>ckpt_max_keep</code> ckpt_max_keep The maximum number of checkpoints to keep during training 5 Integer \\ log_interval The interval of printing logs (unit: epoch) 100 Integer \\ val_while_train Whether to enable the evaluation mode while training True True/False If the value is True, please configure the eval data set synchronously val_start_epoch From which epoch to run the evaluation 1 Interger val_interval Evaluation interval (unit: epoch) 1 Interger drop_overflow_update Whether not updating network parameters when loss/gradient overflows True True/False If value is true, network parameters will not be updated when overflow occurs"},{"location":"tutorials/yaml_configuration/#2-shared-parameters-common","title":"2. Shared parameters (common)","text":"<p>Because the same parameter may need to be reused in different configuration sections, you can customize some common parameters in this section for easy management.</p>"},{"location":"tutorials/yaml_configuration/#3-model-architecture-model","title":"3. Model architecture (model)","text":"<p>In MindOCR, the network architecture of the model is divided into four modules: Transform, Backbone, Neck and Head. For details, please refer to documentation, the following are the configuration instructions and examples of each module.</p> Parameter Description Default Remarks type Network type - Currently supports rec/det; 'rec' means recognition task, 'det' means detection task pretrained Specify pre-trained weight path or url null Supports local checkpoint path or url transform: Transformation method configuration null name Specify transformation method name - Currently supports STN_ON backbone: Backbone network configuration name Specify the backbone network class name or function name - Currently defined classes include rec_resnet34, rec_vgg7, SVTRNet and det_resnet18, det_resnet50, det_resnet152, det_mobilenet_v3. You can also customize new classes, please refer to the documentation for definition. pretrained Whether to load pre-trained backbone weights False Supports bool type or str type to be passed in. If it is True, the default weight will be downloaded and loaded through the url link defined in the backbone py file. If str is passed in, the local checkpoint path or url path can be specified for loading. neck: Network Neck configuration name Neck class name - Currently defined classes include RNNEncoder, DBFPN, EASTFPN and PSEFPN. New classes can also be customized, please refer to the documentation for definition. hidden_size RNN hidden layer unit number - \\ head: Network prediction header configuration name Head class name - Currently supports CTCHead, AttentionHead, DBHead, EASTHead and PSEHead weight_init Set weight initialization 'normal' \\ bias_init Set bias initialization 'zeros' \\ out_channels Set the number of classes - \\ <p>Note: For different networks, the configurable parameters of the backbone/neck/head module will be different. The specific configurable parameters are determined by the init input parameter of the class specified by the <code>name</code> parameter of the module in the above table (For example, assume you specify the neck module is DBFPN. Since the DBFPN class initialization includes adaptive input parameters, parameters such as adaptive can be configured under the model.head in yaml.)</p> <p>Reference example: DBNet, CRNN</p>"},{"location":"tutorials/yaml_configuration/#4-postprocessing-postprocess","title":"4. Postprocessing (postprocess)","text":"<p>Please see the code in mindocr/postprocess</p> Parameter Description Example Remarks name Post-processing class name - Currently supports DBPostprocess, EASTPostprocess, PSEPostprocess, RecCTCLabelDecode and RecAttnLabelDecode character_dict_path Recognition dictionary path None If None, then use the default dictionary [0-9a-z] use_space_char Set whether to add spaces to the dictionary False True/False <p>Note: For different post-processing methods (specified by name), the configurable parameters are different, and are determined by the input parameters of the initialization method <code>__init__</code> of the post-processing class.</p> <p>Reference example: DBNet, PSENet</p>"},{"location":"tutorials/yaml_configuration/#5-evaluation-metrics-metric","title":"5. Evaluation metrics (metric)","text":"<p>Please see the code in mindocr/metrics</p> Parameter Description Default Remarks name Metric class name - Currently supports RecMetric, DetMetric main_indicator Main indicator, used for comparison of optimal models 'hmean' 'acc' for recognition tasks, 'f-score' for detection tasks character_dict_path Recognition dictionary path None If None, then use the default dictionary \"0123456789abcdefghijklmnopqrstuvwxyz\" ignore_space Whether to filter spaces True True/False print_flag Whether to print log False If set True, then output information such as prediction results and standard answers"},{"location":"tutorials/yaml_configuration/#6-loss-function-loss","title":"6. Loss function (loss)","text":"<p>Please see the code in mindocr/losses</p> Parameter Description Default Remarks name loss function name - Currently supports DBLoss, CTCLoss, AttentionLoss, PSEDiceLoss, EASTLoss and CrossEntropySmooth pred_seq_len length of predicted text 26 Determined by network architecture max_label_len The longest label length 25 The value is less than the length of the text predicted by the network batch_size single card batch size 32 \\ <p>Note: For different loss functions (specified by name), the configurable parameters are different and determined by the input parameters of the selected loss function.</p>"},{"location":"tutorials/yaml_configuration/#7-learning-rate-adjustment-strategy-and-optimizer-scheduler-optimizer-loss_scaler","title":"7. Learning rate adjustment strategy and optimizer (scheduler, optimizer, loss_scaler)","text":""},{"location":"tutorials/yaml_configuration/#learning-rate-adjustment-strategy-scheduler","title":"Learning rate adjustment strategy (scheduler)","text":"<p>Please see the code in mindocr/scheduler</p> Parameter Description Default Remarks scheduler Learning rate scheduler name 'constant' Currently supports 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay' min_lr Minimum learning rate 1e-6 Lower lr bound for 'cosine_decay' schedulers. lr Learning rate value 0.01 num_epochs Number of total epochs 200 The number of total epochs for the entire training. warmup_epochs The number of epochs in the training learning rate warmp phase 3 For 'cosine_decay', 'warmup_epochs' indicates the epochs to warmup learning rate from 0 to <code>lr</code>. decay_epochs The number of epochs in the training learning rate decay phase 10 For 'cosine_decay' schedulers, decay LR to min_lr in <code>decay_epochs</code>. For 'step_decay' scheduler, decay LR by a factor of <code>decay_rate</code> every <code>decay_epochs</code>."},{"location":"tutorials/yaml_configuration/#optimizer","title":"optimizer","text":"<p>Please see the code location: mindocr/optim</p> Parameter Description Default Remarks opt Optimizer name 'adam' Currently supports 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'nadam', 'adan', 'rmsprop', 'adagrad', 'lamb'. filter_bias_and_bn Set whether to exclude the weight decrement of bias and batch norm True If True, weight decay will not apply on BN parameters and bias in Conv or Dense layers. momentum momentum 0.9 \\ weight_decay weight decay rate 0 It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. nesterov Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. False True/False"},{"location":"tutorials/yaml_configuration/#loss-scaling-loss_scaler","title":"Loss scaling (loss_scaler)","text":"Parameter Description Default Remarks type Loss scaling method type static Currently supports static, dynamic loss_scale Loss scaling value 1.0 \\ scale_factor When using dynamic loss scaler, the coefficient to dynamically adjust the loss_scale 2.0 At each training step, the loss scaling value is updated to <code>loss_scale</code>/<code>scale_factor</code> when overflow occurs. scale_window When using the dynamic loss scaler, when there is no overflow after the scale_window training step, enlarge the loss_scale by scale_factor times 1000 If the continuous <code>scale_window</code> steps does not overflow, the loss will be increased by <code>loss_scale</code> * <code>scale_factor</code> to update the scaling number"},{"location":"tutorials/yaml_configuration/#8-training-evaluation-and-predict-process-train-eval-predict","title":"8. Training, evaluation and predict process (train, eval, predict)","text":"<p>The configuration of the training process is placed under <code>train</code>, and the configuration of the evaluation phase is placed under <code>eval</code>. Note that during model training, if the training-while-evaluation mode is turned on, that is, when val_while_train=True, an evaluation will be run according to the configuration under <code>eval</code> after each epoch is trained. During the non-training phase, only the <code>eval</code> configuration is read when only running model evaluation.</p>"},{"location":"tutorials/yaml_configuration/#training-process-train","title":"Training process (train)","text":"Parameter Description Default Remarks ckpt_save_dir Set model save path ./tmp_rec \\ resume Resume training after training is interrupted, you can set True/False, or specify the ckpt path that needs to be loaded to resume training False If True, load resume_train.ckpt under the ckpt_save_dir directory to continue training. You can also specify the ckpt file path to load and resume training. dataset_sink_mode Whether the data is directly sinked to the processor for processing - If set to True, the data sinks to the processor, and the data can be returned at least after the end of each epoch gradient_accumulation_steps Number of steps to accumulate the gradients 1 Each step represents a forward calculation, and a reverse correction is performed after the gradient accumulation is completed. clip_grad Whether to clip the gradient False If set to True, gradients are clipped to <code>clip_norm</code> clip_norm The norm of clipping gradient if set clip_grad as True 1 \\ ema Whether to use EMA algorithm False \\ ema_decay EMA decay rate 0.9999 \\ pred_cast_fp32 Whether to cast the data type of logits to fp32 False \\ dataset Dataset configuration For details, please refer to Data document type Dataset class name - Currently supports LMDBDataset, RecDataset and DetDataset dataset_root The root directory of the dataset None Optional data_dir The subdirectory where the dataset is located - If <code>dataset_root</code> is not set, please set this to the full directory label_file The label file path of the dataset - If <code>dataset_root</code> is not set, please set this to the full path, otherwise just set the subpath sample_ratio Data set sampling ratio 1.0 If value &lt; 1.0, random selection shuffle Whether to shuffle the data order True if undering training, otherwise False True/False transform_pipeline Data processing flow None For details, please see transforms output_columns Data loader (data loader) needs to output a list of data attribute names (given to the network/loss calculation/post-processing) (type: list), and the candidate data attribute names are determined by transform_pipeline. None If the value is None, all columns are output. Take crnn as an example, output_columns: ['image', 'text_seq'] net_input_column_index In output_columns, the indices of the input items required by the network construct function [0] \\ label_column_index In output_columns, the indices of the input items required by the loss function [1] \\ loader Data Loading Settings shuffle Whether to shuffle the data order for each epoch True if undering training, otherwise False True/False batch_size Batch size of a single card - \\ drop_remainder Whether to drop the last batch of data when the total data cannot be divided by batch_size True if undering training, otherwise False \\ max_rowsize Specifies the maximum space allocated by shared memory when copying data between multiple processes 64 Default value: 64 num_workers Specifies the number of concurrent processes/threads for batch operations n_cpus / n_devices - 2 This value should be greater than or equal to 2 <p>Reference example: DBNet, CRNN</p>"},{"location":"tutorials/yaml_configuration/#evaluation-process-eval","title":"Evaluation process (eval)","text":"<p>The parameters of <code>eval</code> are basically the same as <code>train</code>, only a few additional parameters are added, and for the rest, please refer to the parameter description of <code>train</code> above.</p> Parameter Usage Default Remarks ckpt_load_path Set model loading path - \\ num_columns_of_labels Set the number of labels in the dataset output columns None If None, assuming the columns after image (data[1:]) are labels. If not None, the num_columns_of_labels columns after image (data[1:1+num_columns_of_labels]) are labels, and the remaining columns are additional info like image_path. drop_remainder Whether to discard the last batch of data when the total number of data cannot be divided by batch_size True if undering training, otherwise False It is recommended to set it to False when doing model evaluation. If it cannot be divisible, mindocr will automatically select a batch size that is the largest divisible"},{"location":"zh/","title":"\u4e3b\u9875","text":""},{"location":"zh/datasets/converters/","title":"\u6570\u636e\u51c6\u5907","text":"<p>This document shows how to convert ocr annotation to the general format (not including LMDB) for model training.</p> <p>You may also refer to <code>convert_datasets.sh</code> which is a quick solution for converting annotation files of all datasets under a given directory.</p> To download and convert OCR datasets to the required data format, please refer to these instructions. <ul> <li>Born-Digital Images</li> <li>CASIA-10K</li> <li>CCPD</li> <li>Chinese text recognition</li> <li>COCO-Text</li> <li>CTW</li> <li>ICDAR2015</li> <li>ICDAR2019 ArT</li> <li>LSVT</li> <li>MLT2017</li> <li>MSRA-TD500</li> <li>MTWI-2018</li> <li>RCTW-17</li> <li>ReCTS</li> <li>SCUT-CTW1500</li> <li>SROIE</li> <li>SVT</li> <li>SynText150k</li> <li>SynthText</li> <li>TextOCR</li> <li>Total-Text</li> </ul>"},{"location":"zh/inference/mindocr_models_list/","title":"MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868","text":""},{"location":"zh/inference/thirdparty_models_list/","title":"\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868","text":""},{"location":"zh/mkdocs/customize_data_transform/","title":"\u5b9a\u5236\u5316\u6570\u636e\u8f6c\u6362","text":""},{"location":"zh/mkdocs/customize_dataset/","title":"\u5b9a\u5236\u5316\u6570\u636e","text":""},{"location":"zh/mkdocs/customize_postprocess/","title":"\u5b9a\u5236\u5316\u540e\u5904\u7406\u65b9\u6cd5","text":""},{"location":"zh/mkdocs/modelzoo_training/","title":"\u8bad\u7ec3","text":""},{"location":"zh/tutorials/frequently_asked_questions/","title":"\u5e38\u89c1\u95ee\u9898","text":""}]}